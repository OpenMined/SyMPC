{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0537d283",
   "metadata": {},
   "source": [
    "<h1> Encrypted Inference-Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a6488",
   "metadata": {},
   "source": [
    "In this tutorial, we train a Linear regression model in plaintext on Boston Housing Dataset. Then we use the model for performing inference on test data. This tutorial uses protocol Falcon for 3 parties and SPDZ for 3 and 5 parties. It depicts how you can perform inference on a dataset with nearly the same accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbe39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a9d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2693cdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd8fc4422d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set a manual seed to maintain consistency\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64573ad",
   "metadata": {},
   "source": [
    "<h2>Data Loading and Processing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a993d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improt dataset\n",
    "dataset=pd.read_csv(\"dataset/Boston.csv\")\n",
    "dataset=dataset.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0c0050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualize and look at columns and rows of dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f893957",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = dataset.drop(\"medv\",axis=1)\n",
    "y_data = dataset[\"medv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b85b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e842f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: medv, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49c397f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419367</td>\n",
       "      <td>0.284548</td>\n",
       "      <td>-1.286636</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.144075</td>\n",
       "      <td>0.413263</td>\n",
       "      <td>-0.119895</td>\n",
       "      <td>0.140075</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.665949</td>\n",
       "      <td>-1.457558</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.074499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.416927</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>0.194082</td>\n",
       "      <td>0.366803</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.491953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.416929</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>1.281446</td>\n",
       "      <td>-0.265549</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.396035</td>\n",
       "      <td>-1.207532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416338</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.015298</td>\n",
       "      <td>-0.809088</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.415751</td>\n",
       "      <td>-1.360171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412074</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.227362</td>\n",
       "      <td>-0.510674</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.025487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>-0.412820</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.438881</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>-0.625178</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.386834</td>\n",
       "      <td>-0.417734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>-0.414839</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>-0.234316</td>\n",
       "      <td>0.288648</td>\n",
       "      <td>-0.715931</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.500355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>-0.413038</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.983986</td>\n",
       "      <td>0.796661</td>\n",
       "      <td>-0.772919</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.982076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>-0.407361</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.724955</td>\n",
       "      <td>0.736268</td>\n",
       "      <td>-0.667776</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.402826</td>\n",
       "      <td>-0.864446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>-0.414590</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>-0.362408</td>\n",
       "      <td>0.434302</td>\n",
       "      <td>-0.612640</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.668397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         crim        zn     indus      chas       nox        rm       age  \\\n",
       "0   -0.419367  0.284548 -1.286636 -0.272329 -0.144075  0.413263 -0.119895   \n",
       "1   -0.416927 -0.487240 -0.592794 -0.272329 -0.739530  0.194082  0.366803   \n",
       "2   -0.416929 -0.487240 -0.592794 -0.272329 -0.739530  1.281446 -0.265549   \n",
       "3   -0.416338 -0.487240 -1.305586 -0.272329 -0.834458  1.015298 -0.809088   \n",
       "4   -0.412074 -0.487240 -1.305586 -0.272329 -0.834458  1.227362 -0.510674   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "501 -0.412820 -0.487240  0.115624 -0.272329  0.157968  0.438881  0.018654   \n",
       "502 -0.414839 -0.487240  0.115624 -0.272329  0.157968 -0.234316  0.288648   \n",
       "503 -0.413038 -0.487240  0.115624 -0.272329  0.157968  0.983986  0.796661   \n",
       "504 -0.407361 -0.487240  0.115624 -0.272329  0.157968  0.724955  0.736268   \n",
       "505 -0.414590 -0.487240  0.115624 -0.272329  0.157968 -0.362408  0.434302   \n",
       "\n",
       "          dis       rad       tax   ptratio     black     lstat  \n",
       "0    0.140075 -0.981871 -0.665949 -1.457558  0.440616 -1.074499  \n",
       "1    0.556609 -0.867024 -0.986353 -0.302794  0.440616 -0.491953  \n",
       "2    0.556609 -0.867024 -0.986353 -0.302794  0.396035 -1.207532  \n",
       "3    1.076671 -0.752178 -1.105022  0.112920  0.415751 -1.360171  \n",
       "4    1.076671 -0.752178 -1.105022  0.112920  0.440616 -1.025487  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "501 -0.625178 -0.981871 -0.802418  1.175303  0.386834 -0.417734  \n",
       "502 -0.715931 -0.981871 -0.802418  1.175303  0.440616 -0.500355  \n",
       "503 -0.772919 -0.981871 -0.802418  1.175303  0.440616 -0.982076  \n",
       "504 -0.667776 -0.981871 -0.802418  1.175303  0.402826 -0.864446  \n",
       "505 -0.612640 -0.981871 -0.802418  1.175303  0.440616 -0.668397  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e938bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(X_data.values.astype(np.float16).astype(np.float32)) \n",
    "targets = torch.tensor(y_data.values.astype(np.float16).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f86c02a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4194,  0.2847, -1.2871,  ..., -1.4580,  0.4407, -1.0742],\n",
       "        [-0.4170, -0.4873, -0.5928,  ..., -0.3027,  0.4407, -0.4919],\n",
       "        [-0.4170, -0.4873, -0.5928,  ..., -0.3027,  0.3960, -1.2080],\n",
       "        ...,\n",
       "        [-0.4131, -0.4873,  0.1156,  ...,  1.1758,  0.4407, -0.9819],\n",
       "        [-0.4075, -0.4873,  0.1156,  ...,  1.1758,  0.4028, -0.8643],\n",
       "        [-0.4146, -0.4873,  0.1156,  ...,  1.1758,  0.4407, -0.6685]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f40e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments of projects\n",
    "batch_size = 16\n",
    "epochs = 500\n",
    "train_test_split = 0.8\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bfae2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features[:int(len(features)*train_test_split)]\n",
    "train_y = targets[:int(len(features)*train_test_split)]\n",
    "\n",
    "test_x = features[int(len(features)*train_test_split)+1:]\n",
    "test_y = targets[int(len(features)*train_test_split)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b13c89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X,y):\n",
    "    batches = []\n",
    "    \n",
    "    for index in range(0,len(train_x)+1,batch_size):\n",
    "        batches.append((X[index:index+batch_size],y[index:index+batch_size]))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "781448fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches=get_batches(train_x,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c8e37",
   "metadata": {},
   "source": [
    "<h1>Plaintext Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cf48b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a2b9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSyNet(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(LinearSyNet, self).__init__(torch_ref=torch_ref)\n",
    "        self.fc1 = self.torch_ref.nn.Linear(13,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c476e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSyNet(torch)\n",
    "criterion = torch.nn.MSELoss(reduction='mean') \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "428cbbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500  Running Loss : 978.0725708007812 and testacc : 367.93927001953125\n",
      "Epoch 1/500  Running Loss : 831.0911865234375 and testacc : 411.168212890625\n",
      "Epoch 2/500  Running Loss : 720.55322265625 and testacc : 440.28497314453125\n",
      "Epoch 3/500  Running Loss : 633.6892700195312 and testacc : 457.1828308105469\n",
      "Epoch 4/500  Running Loss : 562.962646484375 and testacc : 464.29931640625\n",
      "Epoch 5/500  Running Loss : 503.80291748046875 and testacc : 463.9801330566406\n",
      "Epoch 6/500  Running Loss : 453.3403015136719 and testacc : 458.2357177734375\n",
      "Epoch 7/500  Running Loss : 409.6958312988281 and testacc : 448.6855773925781\n",
      "Epoch 8/500  Running Loss : 371.5797424316406 and testacc : 436.5837707519531\n",
      "Epoch 9/500  Running Loss : 338.0628967285156 and testacc : 422.87548828125\n",
      "Epoch 10/500  Running Loss : 308.4455261230469 and testacc : 408.2576904296875\n",
      "Epoch 11/500  Running Loss : 282.1790466308594 and testacc : 393.2343444824219\n",
      "Epoch 12/500  Running Loss : 258.81988525390625 and testacc : 378.1633605957031\n",
      "Epoch 13/500  Running Loss : 238.00051879882812 and testacc : 363.2930603027344\n",
      "Epoch 14/500  Running Loss : 219.4108123779297 and testacc : 348.7910461425781\n",
      "Epoch 15/500  Running Loss : 202.78579711914062 and testacc : 334.7660827636719\n",
      "Epoch 16/500  Running Loss : 187.89691162109375 and testacc : 321.284912109375\n",
      "Epoch 17/500  Running Loss : 174.5458221435547 and testacc : 308.38336181640625\n",
      "Epoch 18/500  Running Loss : 162.55923461914062 and testacc : 296.076416015625\n",
      "Epoch 19/500  Running Loss : 151.7855987548828 and testacc : 284.3641357421875\n",
      "Epoch 20/500  Running Loss : 142.091552734375 and testacc : 273.23583984375\n",
      "Epoch 21/500  Running Loss : 133.3596649169922 and testacc : 262.6748352050781\n",
      "Epoch 22/500  Running Loss : 125.48639678955078 and testacc : 252.6597900390625\n",
      "Epoch 23/500  Running Loss : 118.38018798828125 and testacc : 243.16697692871094\n",
      "Epoch 24/500  Running Loss : 111.96000671386719 and testacc : 234.17144775390625\n",
      "Epoch 25/500  Running Loss : 106.15399932861328 and testacc : 225.6476593017578\n",
      "Epoch 26/500  Running Loss : 100.89836883544922 and testacc : 217.5703582763672\n",
      "Epoch 27/500  Running Loss : 96.13651275634766 and testacc : 209.9147186279297\n",
      "Epoch 28/500  Running Loss : 91.81795501708984 and testacc : 202.65699768066406\n",
      "Epoch 29/500  Running Loss : 87.89775085449219 and testacc : 195.77415466308594\n",
      "Epoch 30/500  Running Loss : 84.33589172363281 and testacc : 189.2443389892578\n",
      "Epoch 31/500  Running Loss : 81.09656524658203 and testacc : 183.04678344726562\n",
      "Epoch 32/500  Running Loss : 78.14794158935547 and testacc : 177.16195678710938\n",
      "Epoch 33/500  Running Loss : 75.46138763427734 and testacc : 171.57127380371094\n",
      "Epoch 34/500  Running Loss : 73.01140594482422 and testacc : 166.25747680664062\n",
      "Epoch 35/500  Running Loss : 70.77510070800781 and testacc : 161.2041473388672\n",
      "Epoch 36/500  Running Loss : 68.7318344116211 and testacc : 156.39573669433594\n",
      "Epoch 37/500  Running Loss : 66.86328125 and testacc : 151.8181915283203\n",
      "Epoch 38/500  Running Loss : 65.15292358398438 and testacc : 147.45770263671875\n",
      "Epoch 39/500  Running Loss : 63.585838317871094 and testacc : 143.3018035888672\n",
      "Epoch 40/500  Running Loss : 62.14863967895508 and testacc : 139.33837890625\n",
      "Epoch 41/500  Running Loss : 60.829307556152344 and testacc : 135.55654907226562\n",
      "Epoch 42/500  Running Loss : 59.61700439453125 and testacc : 131.94569396972656\n",
      "Epoch 43/500  Running Loss : 58.50190734863281 and testacc : 128.4962615966797\n",
      "Epoch 44/500  Running Loss : 57.47517776489258 and testacc : 125.1988525390625\n",
      "Epoch 45/500  Running Loss : 56.52889633178711 and testacc : 122.04502868652344\n",
      "Epoch 46/500  Running Loss : 55.655853271484375 and testacc : 119.02680969238281\n",
      "Epoch 47/500  Running Loss : 54.84951400756836 and testacc : 116.136474609375\n",
      "Epoch 48/500  Running Loss : 54.103973388671875 and testacc : 113.36707305908203\n",
      "Epoch 49/500  Running Loss : 53.41393280029297 and testacc : 110.7120361328125\n",
      "Epoch 50/500  Running Loss : 52.774539947509766 and testacc : 108.1650619506836\n",
      "Epoch 51/500  Running Loss : 52.18141555786133 and testacc : 105.72029113769531\n",
      "Epoch 52/500  Running Loss : 51.63058853149414 and testacc : 103.37239074707031\n",
      "Epoch 53/500  Running Loss : 51.118465423583984 and testacc : 101.11607360839844\n",
      "Epoch 54/500  Running Loss : 50.64175796508789 and testacc : 98.94660949707031\n",
      "Epoch 55/500  Running Loss : 50.197505950927734 and testacc : 96.85958099365234\n",
      "Epoch 56/500  Running Loss : 49.783016204833984 and testacc : 94.8506088256836\n",
      "Epoch 57/500  Running Loss : 49.39579772949219 and testacc : 92.91569519042969\n",
      "Epoch 58/500  Running Loss : 49.03364562988281 and testacc : 91.05118560791016\n",
      "Epoch 59/500  Running Loss : 48.694522857666016 and testacc : 89.25347137451172\n",
      "Epoch 60/500  Running Loss : 48.37654113769531 and testacc : 87.51924133300781\n",
      "Epoch 61/500  Running Loss : 48.0779914855957 and testacc : 85.84532928466797\n",
      "Epoch 62/500  Running Loss : 47.79737854003906 and testacc : 84.22891235351562\n",
      "Epoch 63/500  Running Loss : 47.53329086303711 and testacc : 82.66725158691406\n",
      "Epoch 64/500  Running Loss : 47.284420013427734 and testacc : 81.15760040283203\n",
      "Epoch 65/500  Running Loss : 47.049598693847656 and testacc : 79.69763946533203\n",
      "Epoch 66/500  Running Loss : 46.827754974365234 and testacc : 78.2849349975586\n",
      "Epoch 67/500  Running Loss : 46.617897033691406 and testacc : 76.91742706298828\n",
      "Epoch 68/500  Running Loss : 46.419151306152344 and testacc : 75.59300231933594\n",
      "Epoch 69/500  Running Loss : 46.230655670166016 and testacc : 74.30975341796875\n",
      "Epoch 70/500  Running Loss : 46.051700592041016 and testacc : 73.06578063964844\n",
      "Epoch 71/500  Running Loss : 45.88155746459961 and testacc : 71.8593978881836\n",
      "Epoch 72/500  Running Loss : 45.719608306884766 and testacc : 70.68897247314453\n",
      "Epoch 73/500  Running Loss : 45.56525802612305 and testacc : 69.55294036865234\n",
      "Epoch 74/500  Running Loss : 45.41799545288086 and testacc : 68.44985961914062\n",
      "Epoch 75/500  Running Loss : 45.27731704711914 and testacc : 67.3782958984375\n",
      "Epoch 76/500  Running Loss : 45.14276885986328 and testacc : 66.3370361328125\n",
      "Epoch 77/500  Running Loss : 45.01395797729492 and testacc : 65.32485961914062\n",
      "Epoch 78/500  Running Loss : 44.890472412109375 and testacc : 64.34046936035156\n",
      "Epoch 79/500  Running Loss : 44.77199172973633 and testacc : 63.38288879394531\n",
      "Epoch 80/500  Running Loss : 44.658180236816406 and testacc : 62.45102310180664\n",
      "Epoch 81/500  Running Loss : 44.54874038696289 and testacc : 61.54381561279297\n",
      "Epoch 82/500  Running Loss : 44.44339370727539 and testacc : 60.660343170166016\n",
      "Epoch 83/500  Running Loss : 44.34188461303711 and testacc : 59.79975128173828\n",
      "Epoch 84/500  Running Loss : 44.243988037109375 and testacc : 58.961185455322266\n",
      "Epoch 85/500  Running Loss : 44.14947509765625 and testacc : 58.14378356933594\n",
      "Epoch 86/500  Running Loss : 44.05816650390625 and testacc : 57.34678649902344\n",
      "Epoch 87/500  Running Loss : 43.96985626220703 and testacc : 56.56957244873047\n",
      "Epoch 88/500  Running Loss : 43.88438415527344 and testacc : 55.81130599975586\n",
      "Epoch 89/500  Running Loss : 43.80158996582031 and testacc : 55.071327209472656\n",
      "Epoch 90/500  Running Loss : 43.7213020324707 and testacc : 54.349029541015625\n",
      "Epoch 91/500  Running Loss : 43.6434326171875 and testacc : 53.64389419555664\n",
      "Epoch 92/500  Running Loss : 43.567813873291016 and testacc : 52.95525360107422\n",
      "Epoch 93/500  Running Loss : 43.494346618652344 and testacc : 52.28257369995117\n",
      "Epoch 94/500  Running Loss : 43.42291259765625 and testacc : 51.625404357910156\n",
      "Epoch 95/500  Running Loss : 43.35342025756836 and testacc : 50.98322296142578\n",
      "Epoch 96/500  Running Loss : 43.285762786865234 and testacc : 50.3554801940918\n",
      "Epoch 97/500  Running Loss : 43.21986770629883 and testacc : 49.74178695678711\n",
      "Epoch 98/500  Running Loss : 43.155635833740234 and testacc : 49.14167022705078\n",
      "Epoch 99/500  Running Loss : 43.09299087524414 and testacc : 48.55470657348633\n",
      "Epoch 100/500  Running Loss : 43.0318603515625 and testacc : 47.98057174682617\n",
      "Epoch 101/500  Running Loss : 42.97218704223633 and testacc : 47.418861389160156\n",
      "Epoch 102/500  Running Loss : 42.91390609741211 and testacc : 46.869140625\n",
      "Epoch 103/500  Running Loss : 42.856937408447266 and testacc : 46.33111572265625\n",
      "Epoch 104/500  Running Loss : 42.80125427246094 and testacc : 45.804500579833984\n",
      "Epoch 105/500  Running Loss : 42.746795654296875 and testacc : 45.28887939453125\n",
      "Epoch 106/500  Running Loss : 42.69351577758789 and testacc : 44.784088134765625\n",
      "Epoch 107/500  Running Loss : 42.641353607177734 and testacc : 44.28965759277344\n",
      "Epoch 108/500  Running Loss : 42.59027862548828 and testacc : 43.80537414550781\n",
      "Epoch 109/500  Running Loss : 42.54023361206055 and testacc : 43.331031799316406\n",
      "Epoch 110/500  Running Loss : 42.491207122802734 and testacc : 42.86627960205078\n",
      "Epoch 111/500  Running Loss : 42.44314956665039 and testacc : 42.410919189453125\n",
      "Epoch 112/500  Running Loss : 42.39602279663086 and testacc : 41.96467590332031\n",
      "Epoch 113/500  Running Loss : 42.34979248046875 and testacc : 41.527347564697266\n",
      "Epoch 114/500  Running Loss : 42.304443359375 and testacc : 41.09870147705078\n",
      "Epoch 115/500  Running Loss : 42.259925842285156 and testacc : 40.67849349975586\n",
      "Epoch 116/500  Running Loss : 42.21622848510742 and testacc : 40.26654815673828\n",
      "Epoch 117/500  Running Loss : 42.17331314086914 and testacc : 39.86261749267578\n",
      "Epoch 118/500  Running Loss : 42.131160736083984 and testacc : 39.466590881347656\n",
      "Epoch 119/500  Running Loss : 42.089759826660156 and testacc : 39.0782356262207\n",
      "Epoch 120/500  Running Loss : 42.04906463623047 and testacc : 38.69734573364258\n",
      "Epoch 121/500  Running Loss : 42.00907516479492 and testacc : 38.323787689208984\n",
      "Epoch 122/500  Running Loss : 41.96976089477539 and testacc : 37.95735549926758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/500  Running Loss : 41.93108367919922 and testacc : 37.597877502441406\n",
      "Epoch 124/500  Running Loss : 41.893070220947266 and testacc : 37.24527359008789\n",
      "Epoch 125/500  Running Loss : 41.85566711425781 and testacc : 36.89934539794922\n",
      "Epoch 126/500  Running Loss : 41.818870544433594 and testacc : 36.5599479675293\n",
      "Epoch 127/500  Running Loss : 41.78266525268555 and testacc : 36.22693634033203\n",
      "Epoch 128/500  Running Loss : 41.74702453613281 and testacc : 35.90016174316406\n",
      "Epoch 129/500  Running Loss : 41.71194839477539 and testacc : 35.579463958740234\n",
      "Epoch 130/500  Running Loss : 41.67741775512695 and testacc : 35.264793395996094\n",
      "Epoch 131/500  Running Loss : 41.64341354370117 and testacc : 34.95596694946289\n",
      "Epoch 132/500  Running Loss : 41.609928131103516 and testacc : 34.652870178222656\n",
      "Epoch 133/500  Running Loss : 41.576942443847656 and testacc : 34.35540008544922\n",
      "Epoch 134/500  Running Loss : 41.544464111328125 and testacc : 34.063453674316406\n",
      "Epoch 135/500  Running Loss : 41.5124626159668 and testacc : 33.776878356933594\n",
      "Epoch 136/500  Running Loss : 41.48093032836914 and testacc : 33.49558639526367\n",
      "Epoch 137/500  Running Loss : 41.44984436035156 and testacc : 33.2194938659668\n",
      "Epoch 138/500  Running Loss : 41.419227600097656 and testacc : 32.948429107666016\n",
      "Epoch 139/500  Running Loss : 41.3890380859375 and testacc : 32.68235778808594\n",
      "Epoch 140/500  Running Loss : 41.35927963256836 and testacc : 32.4211540222168\n",
      "Epoch 141/500  Running Loss : 41.32994079589844 and testacc : 32.164710998535156\n",
      "Epoch 142/500  Running Loss : 41.301021575927734 and testacc : 31.912988662719727\n",
      "Epoch 143/500  Running Loss : 41.27251052856445 and testacc : 31.66588592529297\n",
      "Epoch 144/500  Running Loss : 41.244384765625 and testacc : 31.42328453063965\n",
      "Epoch 145/500  Running Loss : 41.21664810180664 and testacc : 31.185121536254883\n",
      "Epoch 146/500  Running Loss : 41.18929672241211 and testacc : 30.95130157470703\n",
      "Epoch 147/500  Running Loss : 41.16231155395508 and testacc : 30.721752166748047\n",
      "Epoch 148/500  Running Loss : 41.13569641113281 and testacc : 30.496389389038086\n",
      "Epoch 149/500  Running Loss : 41.10943603515625 and testacc : 30.275148391723633\n",
      "Epoch 150/500  Running Loss : 41.083518981933594 and testacc : 30.05791473388672\n",
      "Epoch 151/500  Running Loss : 41.05794906616211 and testacc : 29.844669342041016\n",
      "Epoch 152/500  Running Loss : 41.03272247314453 and testacc : 29.635345458984375\n",
      "Epoch 153/500  Running Loss : 41.007835388183594 and testacc : 29.4298038482666\n",
      "Epoch 154/500  Running Loss : 40.98326110839844 and testacc : 29.228036880493164\n",
      "Epoch 155/500  Running Loss : 40.95900344848633 and testacc : 29.029956817626953\n",
      "Epoch 156/500  Running Loss : 40.9350700378418 and testacc : 28.83550262451172\n",
      "Epoch 157/500  Running Loss : 40.911434173583984 and testacc : 28.644594192504883\n",
      "Epoch 158/500  Running Loss : 40.88811492919922 and testacc : 28.45717430114746\n",
      "Epoch 159/500  Running Loss : 40.865074157714844 and testacc : 28.27318572998047\n",
      "Epoch 160/500  Running Loss : 40.842342376708984 and testacc : 28.09259033203125\n",
      "Epoch 161/500  Running Loss : 40.819881439208984 and testacc : 27.91530418395996\n",
      "Epoch 162/500  Running Loss : 40.7977180480957 and testacc : 27.741268157958984\n",
      "Epoch 163/500  Running Loss : 40.775821685791016 and testacc : 27.570449829101562\n",
      "Epoch 164/500  Running Loss : 40.75420379638672 and testacc : 27.40277862548828\n",
      "Epoch 165/500  Running Loss : 40.732845306396484 and testacc : 27.23822593688965\n",
      "Epoch 166/500  Running Loss : 40.71176528930664 and testacc : 27.076698303222656\n",
      "Epoch 167/500  Running Loss : 40.69093704223633 and testacc : 26.91815757751465\n",
      "Epoch 168/500  Running Loss : 40.670372009277344 and testacc : 26.762569427490234\n",
      "Epoch 169/500  Running Loss : 40.650047302246094 and testacc : 26.609878540039062\n",
      "Epoch 170/500  Running Loss : 40.629974365234375 and testacc : 26.460018157958984\n",
      "Epoch 171/500  Running Loss : 40.61014175415039 and testacc : 26.312973022460938\n",
      "Epoch 172/500  Running Loss : 40.59054946899414 and testacc : 26.16868019104004\n",
      "Epoch 173/500  Running Loss : 40.57119369506836 and testacc : 26.027069091796875\n",
      "Epoch 174/500  Running Loss : 40.552066802978516 and testacc : 25.888141632080078\n",
      "Epoch 175/500  Running Loss : 40.533172607421875 and testacc : 25.751798629760742\n",
      "Epoch 176/500  Running Loss : 40.514495849609375 and testacc : 25.61804962158203\n",
      "Epoch 177/500  Running Loss : 40.49604797363281 and testacc : 25.486841201782227\n",
      "Epoch 178/500  Running Loss : 40.47781753540039 and testacc : 25.358123779296875\n",
      "Epoch 179/500  Running Loss : 40.45980453491211 and testacc : 25.23185157775879\n",
      "Epoch 180/500  Running Loss : 40.44198989868164 and testacc : 25.107990264892578\n",
      "Epoch 181/500  Running Loss : 40.424400329589844 and testacc : 24.98651123046875\n",
      "Epoch 182/500  Running Loss : 40.407005310058594 and testacc : 24.867368698120117\n",
      "Epoch 183/500  Running Loss : 40.38981628417969 and testacc : 24.750503540039062\n",
      "Epoch 184/500  Running Loss : 40.372825622558594 and testacc : 24.635879516601562\n",
      "Epoch 185/500  Running Loss : 40.35603713989258 and testacc : 24.52349281311035\n",
      "Epoch 186/500  Running Loss : 40.33943557739258 and testacc : 24.413293838500977\n",
      "Epoch 187/500  Running Loss : 40.323020935058594 and testacc : 24.305246353149414\n",
      "Epoch 188/500  Running Loss : 40.30680847167969 and testacc : 24.199321746826172\n",
      "Epoch 189/500  Running Loss : 40.290767669677734 and testacc : 24.095460891723633\n",
      "Epoch 190/500  Running Loss : 40.27491760253906 and testacc : 23.993669509887695\n",
      "Epoch 191/500  Running Loss : 40.259246826171875 and testacc : 23.893898010253906\n",
      "Epoch 192/500  Running Loss : 40.24375534057617 and testacc : 23.796104431152344\n",
      "Epoch 193/500  Running Loss : 40.228431701660156 and testacc : 23.700260162353516\n",
      "Epoch 194/500  Running Loss : 40.21328353881836 and testacc : 23.606338500976562\n",
      "Epoch 195/500  Running Loss : 40.19831466674805 and testacc : 23.514305114746094\n",
      "Epoch 196/500  Running Loss : 40.18351364135742 and testacc : 23.424110412597656\n",
      "Epoch 197/500  Running Loss : 40.16886901855469 and testacc : 23.33576774597168\n",
      "Epoch 198/500  Running Loss : 40.154388427734375 and testacc : 23.24924087524414\n",
      "Epoch 199/500  Running Loss : 40.140071868896484 and testacc : 23.164461135864258\n",
      "Epoch 200/500  Running Loss : 40.125911712646484 and testacc : 23.08142852783203\n",
      "Epoch 201/500  Running Loss : 40.11191177368164 and testacc : 23.000110626220703\n",
      "Epoch 202/500  Running Loss : 40.09806442260742 and testacc : 22.920501708984375\n",
      "Epoch 203/500  Running Loss : 40.084373474121094 and testacc : 22.84253692626953\n",
      "Epoch 204/500  Running Loss : 40.07083511352539 and testacc : 22.766212463378906\n",
      "Epoch 205/500  Running Loss : 40.057437896728516 and testacc : 22.69149398803711\n",
      "Epoch 206/500  Running Loss : 40.044185638427734 and testacc : 22.618364334106445\n",
      "Epoch 207/500  Running Loss : 40.031089782714844 and testacc : 22.546810150146484\n",
      "Epoch 208/500  Running Loss : 40.01811599731445 and testacc : 22.476770401000977\n",
      "Epoch 209/500  Running Loss : 40.00531005859375 and testacc : 22.40825843811035\n",
      "Epoch 210/500  Running Loss : 39.99263381958008 and testacc : 22.341228485107422\n",
      "Epoch 211/500  Running Loss : 39.9800910949707 and testacc : 22.275659561157227\n",
      "Epoch 212/500  Running Loss : 39.967674255371094 and testacc : 22.211524963378906\n",
      "Epoch 213/500  Running Loss : 39.95540237426758 and testacc : 22.14882469177246\n",
      "Epoch 214/500  Running Loss : 39.943267822265625 and testacc : 22.087520599365234\n",
      "Epoch 215/500  Running Loss : 39.93125915527344 and testacc : 22.0275821685791\n",
      "Epoch 216/500  Running Loss : 39.91938018798828 and testacc : 21.968982696533203\n",
      "Epoch 217/500  Running Loss : 39.90762710571289 and testacc : 21.911720275878906\n",
      "Epoch 218/500  Running Loss : 39.89599609375 and testacc : 21.855758666992188\n",
      "Epoch 219/500  Running Loss : 39.884490966796875 and testacc : 21.80109214782715\n",
      "Epoch 220/500  Running Loss : 39.87311553955078 and testacc : 21.747699737548828\n",
      "Epoch 221/500  Running Loss : 39.861854553222656 and testacc : 21.6955623626709\n",
      "Epoch 222/500  Running Loss : 39.850711822509766 and testacc : 21.644638061523438\n",
      "Epoch 223/500  Running Loss : 39.83968734741211 and testacc : 21.594924926757812\n",
      "Epoch 224/500  Running Loss : 39.82878112792969 and testacc : 21.546396255493164\n",
      "Epoch 225/500  Running Loss : 39.817996978759766 and testacc : 21.49904441833496\n",
      "Epoch 226/500  Running Loss : 39.80730438232422 and testacc : 21.45285415649414\n",
      "Epoch 227/500  Running Loss : 39.79674530029297 and testacc : 21.407787322998047\n",
      "Epoch 228/500  Running Loss : 39.786285400390625 and testacc : 21.363840103149414\n",
      "Epoch 229/500  Running Loss : 39.77593231201172 and testacc : 21.32098960876465\n",
      "Epoch 230/500  Running Loss : 39.76569366455078 and testacc : 21.279216766357422\n",
      "Epoch 231/500  Running Loss : 39.75556182861328 and testacc : 21.238508224487305\n",
      "Epoch 232/500  Running Loss : 39.74553298950195 and testacc : 21.19884490966797\n",
      "Epoch 233/500  Running Loss : 39.73561477661133 and testacc : 21.16020965576172\n",
      "Epoch 234/500  Running Loss : 39.725799560546875 and testacc : 21.122589111328125\n",
      "Epoch 235/500  Running Loss : 39.7160758972168 and testacc : 21.08596420288086\n",
      "Epoch 236/500  Running Loss : 39.70645523071289 and testacc : 21.050317764282227\n",
      "Epoch 237/500  Running Loss : 39.696937561035156 and testacc : 21.015642166137695\n",
      "Epoch 238/500  Running Loss : 39.6875114440918 and testacc : 20.981918334960938\n",
      "Epoch 239/500  Running Loss : 39.67819595336914 and testacc : 20.949127197265625\n",
      "Epoch 240/500  Running Loss : 39.66896057128906 and testacc : 20.917255401611328\n",
      "Epoch 241/500  Running Loss : 39.659828186035156 and testacc : 20.88629150390625\n",
      "Epoch 242/500  Running Loss : 39.65079116821289 and testacc : 20.85620880126953\n",
      "Epoch 243/500  Running Loss : 39.6418342590332 and testacc : 20.826997756958008\n",
      "Epoch 244/500  Running Loss : 39.632972717285156 and testacc : 20.798662185668945\n",
      "Epoch 245/500  Running Loss : 39.62420654296875 and testacc : 20.771162033081055\n",
      "Epoch 246/500  Running Loss : 39.61553192138672 and testacc : 20.744508743286133\n",
      "Epoch 247/500  Running Loss : 39.60693359375 and testacc : 20.71866798400879\n",
      "Epoch 248/500  Running Loss : 39.59843063354492 and testacc : 20.69363021850586\n",
      "Epoch 249/500  Running Loss : 39.59001159667969 and testacc : 20.669389724731445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/500  Running Loss : 39.58168411254883 and testacc : 20.645917892456055\n",
      "Epoch 251/500  Running Loss : 39.573429107666016 and testacc : 20.62322998046875\n",
      "Epoch 252/500  Running Loss : 39.56526565551758 and testacc : 20.601301193237305\n",
      "Epoch 253/500  Running Loss : 39.55717086791992 and testacc : 20.580106735229492\n",
      "Epoch 254/500  Running Loss : 39.54916763305664 and testacc : 20.559650421142578\n",
      "Epoch 255/500  Running Loss : 39.54125213623047 and testacc : 20.53989601135254\n",
      "Epoch 256/500  Running Loss : 39.53340148925781 and testacc : 20.52086639404297\n",
      "Epoch 257/500  Running Loss : 39.525630950927734 and testacc : 20.502532958984375\n",
      "Epoch 258/500  Running Loss : 39.5179443359375 and testacc : 20.484880447387695\n",
      "Epoch 259/500  Running Loss : 39.51033020019531 and testacc : 20.467897415161133\n",
      "Epoch 260/500  Running Loss : 39.50278854370117 and testacc : 20.45157814025879\n",
      "Epoch 261/500  Running Loss : 39.495323181152344 and testacc : 20.4359073638916\n",
      "Epoch 262/500  Running Loss : 39.48793411254883 and testacc : 20.420883178710938\n",
      "Epoch 263/500  Running Loss : 39.48061752319336 and testacc : 20.40648651123047\n",
      "Epoch 264/500  Running Loss : 39.473365783691406 and testacc : 20.3927059173584\n",
      "Epoch 265/500  Running Loss : 39.46619415283203 and testacc : 20.379535675048828\n",
      "Epoch 266/500  Running Loss : 39.45909118652344 and testacc : 20.366954803466797\n",
      "Epoch 267/500  Running Loss : 39.45205307006836 and testacc : 20.3549747467041\n",
      "Epoch 268/500  Running Loss : 39.445091247558594 and testacc : 20.343563079833984\n",
      "Epoch 269/500  Running Loss : 39.43818664550781 and testacc : 20.332725524902344\n",
      "Epoch 270/500  Running Loss : 39.43136215209961 and testacc : 20.322439193725586\n",
      "Epoch 271/500  Running Loss : 39.424591064453125 and testacc : 20.31270980834961\n",
      "Epoch 272/500  Running Loss : 39.41788864135742 and testacc : 20.303518295288086\n",
      "Epoch 273/500  Running Loss : 39.41126251220703 and testacc : 20.294851303100586\n",
      "Epoch 274/500  Running Loss : 39.404685974121094 and testacc : 20.286712646484375\n",
      "Epoch 275/500  Running Loss : 39.398189544677734 and testacc : 20.279090881347656\n",
      "Epoch 276/500  Running Loss : 39.391727447509766 and testacc : 20.271970748901367\n",
      "Epoch 277/500  Running Loss : 39.38535690307617 and testacc : 20.265352249145508\n",
      "Epoch 278/500  Running Loss : 39.37903594970703 and testacc : 20.259214401245117\n",
      "Epoch 279/500  Running Loss : 39.372772216796875 and testacc : 20.253557205200195\n",
      "Epoch 280/500  Running Loss : 39.366573333740234 and testacc : 20.248369216918945\n",
      "Epoch 281/500  Running Loss : 39.360435485839844 and testacc : 20.243650436401367\n",
      "Epoch 282/500  Running Loss : 39.354347229003906 and testacc : 20.239383697509766\n",
      "Epoch 283/500  Running Loss : 39.348331451416016 and testacc : 20.23556137084961\n",
      "Epoch 284/500  Running Loss : 39.34236145019531 and testacc : 20.2321720123291\n",
      "Epoch 285/500  Running Loss : 39.336456298828125 and testacc : 20.229219436645508\n",
      "Epoch 286/500  Running Loss : 39.330596923828125 and testacc : 20.22669219970703\n",
      "Epoch 287/500  Running Loss : 39.32480239868164 and testacc : 20.224578857421875\n",
      "Epoch 288/500  Running Loss : 39.31906509399414 and testacc : 20.222871780395508\n",
      "Epoch 289/500  Running Loss : 39.31336975097656 and testacc : 20.221569061279297\n",
      "Epoch 290/500  Running Loss : 39.307735443115234 and testacc : 20.220651626586914\n",
      "Epoch 291/500  Running Loss : 39.30215835571289 and testacc : 20.220134735107422\n",
      "Epoch 292/500  Running Loss : 39.29661560058594 and testacc : 20.219989776611328\n",
      "Epoch 293/500  Running Loss : 39.29114532470703 and testacc : 20.22022247314453\n",
      "Epoch 294/500  Running Loss : 39.285709381103516 and testacc : 20.220821380615234\n",
      "Epoch 295/500  Running Loss : 39.28034210205078 and testacc : 20.22177505493164\n",
      "Epoch 296/500  Running Loss : 39.2750129699707 and testacc : 20.22309112548828\n",
      "Epoch 297/500  Running Loss : 39.269737243652344 and testacc : 20.224748611450195\n",
      "Epoch 298/500  Running Loss : 39.2645149230957 and testacc : 20.22675132751465\n",
      "Epoch 299/500  Running Loss : 39.25933837890625 and testacc : 20.229074478149414\n",
      "Epoch 300/500  Running Loss : 39.25419998168945 and testacc : 20.23174476623535\n",
      "Epoch 301/500  Running Loss : 39.24912643432617 and testacc : 20.23473358154297\n",
      "Epoch 302/500  Running Loss : 39.244083404541016 and testacc : 20.23802947998047\n",
      "Epoch 303/500  Running Loss : 39.23908996582031 and testacc : 20.24164581298828\n",
      "Epoch 304/500  Running Loss : 39.23415756225586 and testacc : 20.24556541442871\n",
      "Epoch 305/500  Running Loss : 39.229251861572266 and testacc : 20.249784469604492\n",
      "Epoch 306/500  Running Loss : 39.22439956665039 and testacc : 20.25429344177246\n",
      "Epoch 307/500  Running Loss : 39.21958923339844 and testacc : 20.25909423828125\n",
      "Epoch 308/500  Running Loss : 39.21483612060547 and testacc : 20.264179229736328\n",
      "Epoch 309/500  Running Loss : 39.210105895996094 and testacc : 20.269533157348633\n",
      "Epoch 310/500  Running Loss : 39.20543670654297 and testacc : 20.27516746520996\n",
      "Epoch 311/500  Running Loss : 39.200801849365234 and testacc : 20.28106689453125\n",
      "Epoch 312/500  Running Loss : 39.19620132446289 and testacc : 20.28721809387207\n",
      "Epoch 313/500  Running Loss : 39.191654205322266 and testacc : 20.293636322021484\n",
      "Epoch 314/500  Running Loss : 39.18714141845703 and testacc : 20.300304412841797\n",
      "Epoch 315/500  Running Loss : 39.18266677856445 and testacc : 20.307220458984375\n",
      "Epoch 316/500  Running Loss : 39.17824935913086 and testacc : 20.314380645751953\n",
      "Epoch 317/500  Running Loss : 39.173858642578125 and testacc : 20.3217716217041\n",
      "Epoch 318/500  Running Loss : 39.16950607299805 and testacc : 20.32940101623535\n",
      "Epoch 319/500  Running Loss : 39.165199279785156 and testacc : 20.337257385253906\n",
      "Epoch 320/500  Running Loss : 39.160919189453125 and testacc : 20.34532928466797\n",
      "Epoch 321/500  Running Loss : 39.15670394897461 and testacc : 20.35363006591797\n",
      "Epoch 322/500  Running Loss : 39.15250015258789 and testacc : 20.36213493347168\n",
      "Epoch 323/500  Running Loss : 39.148345947265625 and testacc : 20.3708553314209\n",
      "Epoch 324/500  Running Loss : 39.144222259521484 and testacc : 20.379789352416992\n",
      "Epoch 325/500  Running Loss : 39.140140533447266 and testacc : 20.388916015625\n",
      "Epoch 326/500  Running Loss : 39.13609313964844 and testacc : 20.398235321044922\n",
      "Epoch 327/500  Running Loss : 39.132080078125 and testacc : 20.407751083374023\n",
      "Epoch 328/500  Running Loss : 39.128108978271484 and testacc : 20.417455673217773\n",
      "Epoch 329/500  Running Loss : 39.12416458129883 and testacc : 20.427345275878906\n",
      "Epoch 330/500  Running Loss : 39.120262145996094 and testacc : 20.43741798400879\n",
      "Epoch 331/500  Running Loss : 39.11638641357422 and testacc : 20.447662353515625\n",
      "Epoch 332/500  Running Loss : 39.112548828125 and testacc : 20.458087921142578\n",
      "Epoch 333/500  Running Loss : 39.10874557495117 and testacc : 20.46868324279785\n",
      "Epoch 334/500  Running Loss : 39.104976654052734 and testacc : 20.47944450378418\n",
      "Epoch 335/500  Running Loss : 39.10124206542969 and testacc : 20.490375518798828\n",
      "Epoch 336/500  Running Loss : 39.09754180908203 and testacc : 20.501455307006836\n",
      "Epoch 337/500  Running Loss : 39.09386444091797 and testacc : 20.5126895904541\n",
      "Epoch 338/500  Running Loss : 39.090232849121094 and testacc : 20.524080276489258\n",
      "Epoch 339/500  Running Loss : 39.08661651611328 and testacc : 20.535627365112305\n",
      "Epoch 340/500  Running Loss : 39.08304977416992 and testacc : 20.547306060791016\n",
      "Epoch 341/500  Running Loss : 39.07950210571289 and testacc : 20.55913543701172\n",
      "Epoch 342/500  Running Loss : 39.07598114013672 and testacc : 20.57109832763672\n",
      "Epoch 343/500  Running Loss : 39.0724983215332 and testacc : 20.583200454711914\n",
      "Epoch 344/500  Running Loss : 39.06904220581055 and testacc : 20.595434188842773\n",
      "Epoch 345/500  Running Loss : 39.06562042236328 and testacc : 20.607799530029297\n",
      "Epoch 346/500  Running Loss : 39.06222915649414 and testacc : 20.620298385620117\n",
      "Epoch 347/500  Running Loss : 39.05885314941406 and testacc : 20.632902145385742\n",
      "Epoch 348/500  Running Loss : 39.05553436279297 and testacc : 20.64563751220703\n",
      "Epoch 349/500  Running Loss : 39.05221939086914 and testacc : 20.658483505249023\n",
      "Epoch 350/500  Running Loss : 39.0489387512207 and testacc : 20.67145347595215\n",
      "Epoch 351/500  Running Loss : 39.04568862915039 and testacc : 20.684532165527344\n",
      "Epoch 352/500  Running Loss : 39.0424690246582 and testacc : 20.69772720336914\n",
      "Epoch 353/500  Running Loss : 39.03926467895508 and testacc : 20.71101951599121\n",
      "Epoch 354/500  Running Loss : 39.03609848022461 and testacc : 20.724411010742188\n",
      "Epoch 355/500  Running Loss : 39.032955169677734 and testacc : 20.73789405822754\n",
      "Epoch 356/500  Running Loss : 39.02984619140625 and testacc : 20.751489639282227\n",
      "Epoch 357/500  Running Loss : 39.02676010131836 and testacc : 20.765193939208984\n",
      "Epoch 358/500  Running Loss : 39.023685455322266 and testacc : 20.778980255126953\n",
      "Epoch 359/500  Running Loss : 39.020652770996094 and testacc : 20.792858123779297\n",
      "Epoch 360/500  Running Loss : 39.01765060424805 and testacc : 20.80681037902832\n",
      "Epoch 361/500  Running Loss : 39.01465606689453 and testacc : 20.820865631103516\n",
      "Epoch 362/500  Running Loss : 39.011695861816406 and testacc : 20.83498191833496\n",
      "Epoch 363/500  Running Loss : 39.008766174316406 and testacc : 20.84920883178711\n",
      "Epoch 364/500  Running Loss : 39.005859375 and testacc : 20.863489151000977\n",
      "Epoch 365/500  Running Loss : 39.00296401977539 and testacc : 20.877857208251953\n",
      "Epoch 366/500  Running Loss : 39.000099182128906 and testacc : 20.89231300354004\n",
      "Epoch 367/500  Running Loss : 38.99726867675781 and testacc : 20.906829833984375\n",
      "Epoch 368/500  Running Loss : 38.99444580078125 and testacc : 20.921415328979492\n",
      "Epoch 369/500  Running Loss : 38.991661071777344 and testacc : 20.936071395874023\n",
      "Epoch 370/500  Running Loss : 38.9888916015625 and testacc : 20.950786590576172\n",
      "Epoch 371/500  Running Loss : 38.986148834228516 and testacc : 20.965574264526367\n",
      "Epoch 372/500  Running Loss : 38.983421325683594 and testacc : 20.98041534423828\n",
      "Epoch 373/500  Running Loss : 38.98072052001953 and testacc : 20.99534034729004\n",
      "Epoch 374/500  Running Loss : 38.97804260253906 and testacc : 21.01030731201172\n",
      "Epoch 375/500  Running Loss : 38.97539138793945 and testacc : 21.025327682495117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376/500  Running Loss : 38.97275161743164 and testacc : 21.0404109954834\n",
      "Epoch 377/500  Running Loss : 38.970130920410156 and testacc : 21.05553436279297\n",
      "Epoch 378/500  Running Loss : 38.9675407409668 and testacc : 21.070709228515625\n",
      "Epoch 379/500  Running Loss : 38.9649772644043 and testacc : 21.0859375\n",
      "Epoch 380/500  Running Loss : 38.96242904663086 and testacc : 21.10120964050293\n",
      "Epoch 381/500  Running Loss : 38.959896087646484 and testacc : 21.116533279418945\n",
      "Epoch 382/500  Running Loss : 38.9573860168457 and testacc : 21.131898880004883\n",
      "Epoch 383/500  Running Loss : 38.95490646362305 and testacc : 21.14729881286621\n",
      "Epoch 384/500  Running Loss : 38.95243453979492 and testacc : 21.162734985351562\n",
      "Epoch 385/500  Running Loss : 38.94998550415039 and testacc : 21.178211212158203\n",
      "Epoch 386/500  Running Loss : 38.94756317138672 and testacc : 21.193729400634766\n",
      "Epoch 387/500  Running Loss : 38.945152282714844 and testacc : 21.209272384643555\n",
      "Epoch 388/500  Running Loss : 38.94276809692383 and testacc : 21.2248477935791\n",
      "Epoch 389/500  Running Loss : 38.940399169921875 and testacc : 21.240463256835938\n",
      "Epoch 390/500  Running Loss : 38.938053131103516 and testacc : 21.256101608276367\n",
      "Epoch 391/500  Running Loss : 38.935726165771484 and testacc : 21.271774291992188\n",
      "Epoch 392/500  Running Loss : 38.93341064453125 and testacc : 21.28748321533203\n",
      "Epoch 393/500  Running Loss : 38.931114196777344 and testacc : 21.303213119506836\n",
      "Epoch 394/500  Running Loss : 38.92884063720703 and testacc : 21.318967819213867\n",
      "Epoch 395/500  Running Loss : 38.92658233642578 and testacc : 21.334760665893555\n",
      "Epoch 396/500  Running Loss : 38.924339294433594 and testacc : 21.350555419921875\n",
      "Epoch 397/500  Running Loss : 38.922122955322266 and testacc : 21.366374969482422\n",
      "Epoch 398/500  Running Loss : 38.919918060302734 and testacc : 21.382217407226562\n",
      "Epoch 399/500  Running Loss : 38.917728424072266 and testacc : 21.398086547851562\n",
      "Epoch 400/500  Running Loss : 38.91556167602539 and testacc : 21.413959503173828\n",
      "Epoch 401/500  Running Loss : 38.913414001464844 and testacc : 21.429841995239258\n",
      "Epoch 402/500  Running Loss : 38.911277770996094 and testacc : 21.44574737548828\n",
      "Epoch 403/500  Running Loss : 38.909156799316406 and testacc : 21.461660385131836\n",
      "Epoch 404/500  Running Loss : 38.90705871582031 and testacc : 21.477609634399414\n",
      "Epoch 405/500  Running Loss : 38.90497589111328 and testacc : 21.49355125427246\n",
      "Epoch 406/500  Running Loss : 38.90290069580078 and testacc : 21.509506225585938\n",
      "Epoch 407/500  Running Loss : 38.900856018066406 and testacc : 21.52546501159668\n",
      "Epoch 408/500  Running Loss : 38.89881896972656 and testacc : 21.54142951965332\n",
      "Epoch 409/500  Running Loss : 38.896793365478516 and testacc : 21.557397842407227\n",
      "Epoch 410/500  Running Loss : 38.89479064941406 and testacc : 21.573379516601562\n",
      "Epoch 411/500  Running Loss : 38.89280319213867 and testacc : 21.5893611907959\n",
      "Epoch 412/500  Running Loss : 38.890830993652344 and testacc : 21.60536766052246\n",
      "Epoch 413/500  Running Loss : 38.88886642456055 and testacc : 21.621353149414062\n",
      "Epoch 414/500  Running Loss : 38.88692092895508 and testacc : 21.637344360351562\n",
      "Epoch 415/500  Running Loss : 38.885005950927734 and testacc : 21.653339385986328\n",
      "Epoch 416/500  Running Loss : 38.88309097290039 and testacc : 21.6693172454834\n",
      "Epoch 417/500  Running Loss : 38.881195068359375 and testacc : 21.685321807861328\n",
      "Epoch 418/500  Running Loss : 38.87931442260742 and testacc : 21.70130729675293\n",
      "Epoch 419/500  Running Loss : 38.877437591552734 and testacc : 21.717300415039062\n",
      "Epoch 420/500  Running Loss : 38.875587463378906 and testacc : 21.7332706451416\n",
      "Epoch 421/500  Running Loss : 38.873748779296875 and testacc : 21.74924659729004\n",
      "Epoch 422/500  Running Loss : 38.871917724609375 and testacc : 21.765233993530273\n",
      "Epoch 423/500  Running Loss : 38.8701057434082 and testacc : 21.781192779541016\n",
      "Epoch 424/500  Running Loss : 38.868309020996094 and testacc : 21.797147750854492\n",
      "Epoch 425/500  Running Loss : 38.86652755737305 and testacc : 21.813098907470703\n",
      "Epoch 426/500  Running Loss : 38.86475372314453 and testacc : 21.829029083251953\n",
      "Epoch 427/500  Running Loss : 38.863006591796875 and testacc : 21.84494972229004\n",
      "Epoch 428/500  Running Loss : 38.86125183105469 and testacc : 21.860876083374023\n",
      "Epoch 429/500  Running Loss : 38.859527587890625 and testacc : 21.876792907714844\n",
      "Epoch 430/500  Running Loss : 38.85780334472656 and testacc : 21.892681121826172\n",
      "Epoch 431/500  Running Loss : 38.856101989746094 and testacc : 21.90856170654297\n",
      "Epoch 432/500  Running Loss : 38.85441207885742 and testacc : 21.924440383911133\n",
      "Epoch 433/500  Running Loss : 38.85272979736328 and testacc : 21.94029426574707\n",
      "Epoch 434/500  Running Loss : 38.85105895996094 and testacc : 21.956125259399414\n",
      "Epoch 435/500  Running Loss : 38.84941101074219 and testacc : 21.971942901611328\n",
      "Epoch 436/500  Running Loss : 38.8477668762207 and testacc : 21.987751007080078\n",
      "Epoch 437/500  Running Loss : 38.84613800048828 and testacc : 22.00353240966797\n",
      "Epoch 438/500  Running Loss : 38.844520568847656 and testacc : 22.019304275512695\n",
      "Epoch 439/500  Running Loss : 38.842918395996094 and testacc : 22.035057067871094\n",
      "Epoch 440/500  Running Loss : 38.84132766723633 and testacc : 22.050796508789062\n",
      "Epoch 441/500  Running Loss : 38.839752197265625 and testacc : 22.06649398803711\n",
      "Epoch 442/500  Running Loss : 38.83818054199219 and testacc : 22.082195281982422\n",
      "Epoch 443/500  Running Loss : 38.83662414550781 and testacc : 22.097848892211914\n",
      "Epoch 444/500  Running Loss : 38.8350715637207 and testacc : 22.1135196685791\n",
      "Epoch 445/500  Running Loss : 38.83354187011719 and testacc : 22.129133224487305\n",
      "Epoch 446/500  Running Loss : 38.8320198059082 and testacc : 22.144758224487305\n",
      "Epoch 447/500  Running Loss : 38.83050537109375 and testacc : 22.160348892211914\n",
      "Epoch 448/500  Running Loss : 38.829010009765625 and testacc : 22.17591094970703\n",
      "Epoch 449/500  Running Loss : 38.82752227783203 and testacc : 22.191442489624023\n",
      "Epoch 450/500  Running Loss : 38.82603454589844 and testacc : 22.20697021484375\n",
      "Epoch 451/500  Running Loss : 38.824581146240234 and testacc : 22.222454071044922\n",
      "Epoch 452/500  Running Loss : 38.823116302490234 and testacc : 22.237926483154297\n",
      "Epoch 453/500  Running Loss : 38.8216667175293 and testacc : 22.253389358520508\n",
      "Epoch 454/500  Running Loss : 38.82023239135742 and testacc : 22.268815994262695\n",
      "Epoch 455/500  Running Loss : 38.818809509277344 and testacc : 22.284194946289062\n",
      "Epoch 456/500  Running Loss : 38.8173942565918 and testacc : 22.299558639526367\n",
      "Epoch 457/500  Running Loss : 38.81599044799805 and testacc : 22.314889907836914\n",
      "Epoch 458/500  Running Loss : 38.81459426879883 and testacc : 22.3301944732666\n",
      "Epoch 459/500  Running Loss : 38.813209533691406 and testacc : 22.345476150512695\n",
      "Epoch 460/500  Running Loss : 38.81184005737305 and testacc : 22.36073875427246\n",
      "Epoch 461/500  Running Loss : 38.81047439575195 and testacc : 22.375946044921875\n",
      "Epoch 462/500  Running Loss : 38.809120178222656 and testacc : 22.39113426208496\n",
      "Epoch 463/500  Running Loss : 38.80777359008789 and testacc : 22.406280517578125\n",
      "Epoch 464/500  Running Loss : 38.80644607543945 and testacc : 22.421424865722656\n",
      "Epoch 465/500  Running Loss : 38.80511474609375 and testacc : 22.43653106689453\n",
      "Epoch 466/500  Running Loss : 38.80379867553711 and testacc : 22.451595306396484\n",
      "Epoch 467/500  Running Loss : 38.80250549316406 and testacc : 22.466636657714844\n",
      "Epoch 468/500  Running Loss : 38.80119705200195 and testacc : 22.481653213500977\n",
      "Epoch 469/500  Running Loss : 38.79991912841797 and testacc : 22.49663543701172\n",
      "Epoch 470/500  Running Loss : 38.79864501953125 and testacc : 22.51158332824707\n",
      "Epoch 471/500  Running Loss : 38.79736328125 and testacc : 22.526504516601562\n",
      "Epoch 472/500  Running Loss : 38.796104431152344 and testacc : 22.541378021240234\n",
      "Epoch 473/500  Running Loss : 38.79486083984375 and testacc : 22.556224822998047\n",
      "Epoch 474/500  Running Loss : 38.793609619140625 and testacc : 22.571043014526367\n",
      "Epoch 475/500  Running Loss : 38.792381286621094 and testacc : 22.58580780029297\n",
      "Epoch 476/500  Running Loss : 38.79115295410156 and testacc : 22.600570678710938\n",
      "Epoch 477/500  Running Loss : 38.78994369506836 and testacc : 22.615272521972656\n",
      "Epoch 478/500  Running Loss : 38.78873825073242 and testacc : 22.62995719909668\n",
      "Epoch 479/500  Running Loss : 38.78753662109375 and testacc : 22.644622802734375\n",
      "Epoch 480/500  Running Loss : 38.78635025024414 and testacc : 22.65923500061035\n",
      "Epoch 481/500  Running Loss : 38.78517150878906 and testacc : 22.673810958862305\n",
      "Epoch 482/500  Running Loss : 38.78398895263672 and testacc : 22.688365936279297\n",
      "Epoch 483/500  Running Loss : 38.7828254699707 and testacc : 22.702863693237305\n",
      "Epoch 484/500  Running Loss : 38.78167724609375 and testacc : 22.71734046936035\n",
      "Epoch 485/500  Running Loss : 38.780517578125 and testacc : 22.731788635253906\n",
      "Epoch 486/500  Running Loss : 38.779380798339844 and testacc : 22.746173858642578\n",
      "Epoch 487/500  Running Loss : 38.77825164794922 and testacc : 22.760513305664062\n",
      "Epoch 488/500  Running Loss : 38.77713394165039 and testacc : 22.774856567382812\n",
      "Epoch 489/500  Running Loss : 38.77600860595703 and testacc : 22.78914451599121\n",
      "Epoch 490/500  Running Loss : 38.77490234375 and testacc : 22.803403854370117\n",
      "Epoch 491/500  Running Loss : 38.773799896240234 and testacc : 22.817617416381836\n",
      "Epoch 492/500  Running Loss : 38.772705078125 and testacc : 22.831798553466797\n",
      "Epoch 493/500  Running Loss : 38.77162551879883 and testacc : 22.845949172973633\n",
      "Epoch 494/500  Running Loss : 38.770545959472656 and testacc : 22.86005210876465\n",
      "Epoch 495/500  Running Loss : 38.76947784423828 and testacc : 22.874095916748047\n",
      "Epoch 496/500  Running Loss : 38.76841354370117 and testacc : 22.88814926147461\n",
      "Epoch 497/500  Running Loss : 38.767356872558594 and testacc : 22.902141571044922\n",
      "Epoch 498/500  Running Loss : 38.76630783081055 and testacc : 22.91607666015625\n",
      "Epoch 499/500  Running Loss : 38.76526641845703 and testacc : 22.930004119873047\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  running_loss = 0.0\n",
    "  for index in range(0,len(train_batches)):\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(train_batches[index][0]).reshape([-1])\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs,train_batches[index][1])\n",
    "    running_loss += loss\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "  test_accuracy = outputs = criterion(model(test_x).reshape([-1]),test_y)\n",
    "  print(f\"Epoch {epoch}/{epochs}  Running Loss : {running_loss.item()/batch_size} and testacc : {test_accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ad352",
   "metadata": {},
   "source": [
    "<h1>Plaintext Inference</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0869e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "plaintext_predictions = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc3fbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  63.399288177490234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrishikesh/opt/anaconda3/envs/sympc/lib/python3.9/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101])) that is different to the input size (torch.Size([101, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE Loss: \",criterion(plaintext_predictions,test_y).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b8c96",
   "metadata": {},
   "source": [
    "<h1>Encrypted Inference</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "314395b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from sympc.module.nn import mse_loss\n",
    "import sympc\n",
    "from sympc.session import Session\n",
    "from sympc.session import SessionManager\n",
    "from sympc.tensor import MPCTensor\n",
    "from sympc.optim import SGD\n",
    "from sympc.config import Config\n",
    "from sympc.protocol import Falcon,FSS\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e9096e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clients(n_parties):\n",
    "  # Define the virtual machines that would be use in the computation\n",
    "  parties=[]\n",
    "\n",
    "  for index in range(n_parties): \n",
    "      parties.append(sy.VirtualMachine(name = \"worker\"+str(index)).get_root_client())\n",
    "\n",
    "  return parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a394a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(n_clients,protocol):\n",
    "\n",
    "  parties=get_clients(n_clients)\n",
    "\n",
    "  # Setup the session for the computation\n",
    "  session = Session(parties = parties,protocol = protocol)\n",
    "  SessionManager.setup_mpc(session)\n",
    "\n",
    "  mpc_model = model.share(session)\n",
    "\n",
    "  test_data=MPCTensor(secret=test_x, session = session)\n",
    "\n",
    "  start_time = time.time()\n",
    "  enc_results = mpc_model(test_data)\n",
    "  end_time = time.time()\n",
    "\n",
    "  print(f\"Time for inference: {end_time-start_time}s\")\n",
    "\n",
    "  predictions = enc_results.reconstruct()\n",
    "    \n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d62c0ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.033750057220458984s\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,Falcon(\"semi-honest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0eb8b3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  tensor(63.3991)\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE Loss: \",criterion(predictions,test_y).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5cdd5",
   "metadata": {},
   "source": [
    "We can see that the prediction values and mean squared error values are almost the same as final model. Small differences are due to precision loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6be0ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0\n",
      "Encrypted Prediction Output 1.8402252197265625\n",
      "Plaintext Prediction Output 1.8402048349380493\n",
      "Expected Prediction: 5.0\n",
      "\n",
      "\n",
      "Index 1\n",
      "Encrypted Prediction Output 5.19158935546875\n",
      "Plaintext Prediction Output 5.191580772399902\n",
      "Expected Prediction: 11.8984375\n",
      "\n",
      "\n",
      "Index 2\n",
      "Encrypted Prediction Output 19.786148071289062\n",
      "Plaintext Prediction Output 19.786155700683594\n",
      "Expected Prediction: 27.90625\n",
      "\n",
      "\n",
      "Index 3\n",
      "Encrypted Prediction Output 13.1734619140625\n",
      "Plaintext Prediction Output 13.173490524291992\n",
      "Expected Prediction: 17.203125\n",
      "\n",
      "\n",
      "Index 4\n",
      "Encrypted Prediction Output 20.987045288085938\n",
      "Plaintext Prediction Output 20.987075805664062\n",
      "Expected Prediction: 27.5\n",
      "\n",
      "\n",
      "Index 5\n",
      "Encrypted Prediction Output 14.210540771484375\n",
      "Plaintext Prediction Output 14.210532188415527\n",
      "Expected Prediction: 15.0\n",
      "\n",
      "\n",
      "Index 6\n",
      "Encrypted Prediction Output 19.272994995117188\n",
      "Plaintext Prediction Output 19.273014068603516\n",
      "Expected Prediction: 17.203125\n",
      "\n",
      "\n",
      "Index 7\n",
      "Encrypted Prediction Output 1.7180023193359375\n",
      "Plaintext Prediction Output 1.717998743057251\n",
      "Expected Prediction: 17.90625\n",
      "\n",
      "\n",
      "Index 8\n",
      "Encrypted Prediction Output 10.214675903320312\n",
      "Plaintext Prediction Output 10.214686393737793\n",
      "Expected Prediction: 16.296875\n",
      "\n",
      "\n",
      "Index 9\n",
      "Encrypted Prediction Output -7.1717681884765625\n",
      "Plaintext Prediction Output -7.171802043914795\n",
      "Expected Prediction: 7.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(0,10):\n",
    "    print(f\"Index {index}\")\n",
    "    print(f\"Encrypted Prediction Output {predictions[index].item()}\")\n",
    "    print(f\"Plaintext Prediction Output {plaintext_predictions[index].item()}\")\n",
    "    print(f\"Expected Prediction: {test_y[index]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc2029",
   "metadata": {},
   "source": [
    "Falcon can also work with malicious security guarantee but at a large inference time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c930a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.5756688117980957s\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,Falcon(\"malicious\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "598ab19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-13T17:54:24.721347+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: e43609e4f3b24af1bab46d6a3463c6cf>.\n",
      "[2021-07-13T17:54:24.724769+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 433768ff3ded40c586a6b517735d8271>.\n",
      "[2021-07-13T17:54:24.726637+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 47ca2a8b0f55424b8c7dee7dc0a0ccb0>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.5407030582427979s\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,FSS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8ceef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-13T17:54:26.588460+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 384b45903a294c2e85c2b480de81ce4d>.\n",
      "[2021-07-13T17:54:26.595719+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 35ffa19d3ebf42ce9a3c5a0a619d59a9>.\n",
      "[2021-07-13T17:54:26.597316+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 728a7f55e01c4ff98525a9b84f6a1cff>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.9237020015716553s\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(5,FSS())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04560264",
   "metadata": {},
   "source": [
    "\n",
    "<center> <h3> Comparison </h3> </center>\n",
    "\n",
    "| Protocol | Security Type| Parties | Inference Time (s) |\n",
    "| --- | --- | --- | --- |\n",
    "| Falcon | Semi-honest | 3 | 0.03391 |\n",
    "| Falcon | Malicious | 3 | 0.61146 |\n",
    "| FSS| Semi-honest | 3 | 0.56047 |\n",
    "| FSS | Semi-honest | 5 | 0.91995|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9830545",
   "metadata": {},
   "source": [
    "Falcon works for 3 parties and semi honest security setting provides significant security guarantee. For, N number of parties you can Functional Secret Sharing protocol (FSS). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42f165",
   "metadata": {},
   "source": [
    "<h3>What's next?</h3>\n",
    "\n",
    "SyMPC is still under development! We will add here more features as soon they are stable enough, stay tuned! 🕺\n",
    "\n",
    "If you enjoyed this tutorial, show your support by Starring SyMPC! 🙏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049acf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
