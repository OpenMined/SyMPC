{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0537d283",
   "metadata": {},
   "source": [
    "<h1> Encrypted Inference-Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a6488",
   "metadata": {},
   "source": [
    "In this tutorial, we train a Linear regression model in plaintext on Boston Housing Dataset. Then we use the model for performing inference on test data. This tutorial uses protocol Falcon for 3 parties and SPDZ for 3 and 5 parties. It depicts how you can perform inference on a dataset with nearly the same accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbe39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a9d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2693cdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd8fc4422d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set a manual seed to maintain consistency\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64573ad",
   "metadata": {},
   "source": [
    "<h2>Data Loading and Processing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a993d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improt dataset\n",
    "dataset=pd.read_csv(\"dataset/Boston.csv\")\n",
    "dataset=dataset.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0c0050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualize and look at columns and rows of dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f893957",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = dataset.drop(\"medv\",axis=1)\n",
    "y_data = dataset[\"medv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b85b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e842f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: medv, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49c397f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419367</td>\n",
       "      <td>0.284548</td>\n",
       "      <td>-1.286636</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.144075</td>\n",
       "      <td>0.413263</td>\n",
       "      <td>-0.119895</td>\n",
       "      <td>0.140075</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.665949</td>\n",
       "      <td>-1.457558</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.074499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.416927</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>0.194082</td>\n",
       "      <td>0.366803</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.491953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.416929</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>1.281446</td>\n",
       "      <td>-0.265549</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.396035</td>\n",
       "      <td>-1.207532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416338</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.015298</td>\n",
       "      <td>-0.809088</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.415751</td>\n",
       "      <td>-1.360171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412074</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.227362</td>\n",
       "      <td>-0.510674</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.025487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>-0.412820</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.438881</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>-0.625178</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.386834</td>\n",
       "      <td>-0.417734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>-0.414839</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>-0.234316</td>\n",
       "      <td>0.288648</td>\n",
       "      <td>-0.715931</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.500355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>-0.413038</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.983986</td>\n",
       "      <td>0.796661</td>\n",
       "      <td>-0.772919</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.982076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>-0.407361</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.724955</td>\n",
       "      <td>0.736268</td>\n",
       "      <td>-0.667776</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.402826</td>\n",
       "      <td>-0.864446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>-0.414590</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>-0.362408</td>\n",
       "      <td>0.434302</td>\n",
       "      <td>-0.612640</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.668397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         crim        zn     indus      chas       nox        rm       age  \\\n",
       "0   -0.419367  0.284548 -1.286636 -0.272329 -0.144075  0.413263 -0.119895   \n",
       "1   -0.416927 -0.487240 -0.592794 -0.272329 -0.739530  0.194082  0.366803   \n",
       "2   -0.416929 -0.487240 -0.592794 -0.272329 -0.739530  1.281446 -0.265549   \n",
       "3   -0.416338 -0.487240 -1.305586 -0.272329 -0.834458  1.015298 -0.809088   \n",
       "4   -0.412074 -0.487240 -1.305586 -0.272329 -0.834458  1.227362 -0.510674   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "501 -0.412820 -0.487240  0.115624 -0.272329  0.157968  0.438881  0.018654   \n",
       "502 -0.414839 -0.487240  0.115624 -0.272329  0.157968 -0.234316  0.288648   \n",
       "503 -0.413038 -0.487240  0.115624 -0.272329  0.157968  0.983986  0.796661   \n",
       "504 -0.407361 -0.487240  0.115624 -0.272329  0.157968  0.724955  0.736268   \n",
       "505 -0.414590 -0.487240  0.115624 -0.272329  0.157968 -0.362408  0.434302   \n",
       "\n",
       "          dis       rad       tax   ptratio     black     lstat  \n",
       "0    0.140075 -0.981871 -0.665949 -1.457558  0.440616 -1.074499  \n",
       "1    0.556609 -0.867024 -0.986353 -0.302794  0.440616 -0.491953  \n",
       "2    0.556609 -0.867024 -0.986353 -0.302794  0.396035 -1.207532  \n",
       "3    1.076671 -0.752178 -1.105022  0.112920  0.415751 -1.360171  \n",
       "4    1.076671 -0.752178 -1.105022  0.112920  0.440616 -1.025487  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "501 -0.625178 -0.981871 -0.802418  1.175303  0.386834 -0.417734  \n",
       "502 -0.715931 -0.981871 -0.802418  1.175303  0.440616 -0.500355  \n",
       "503 -0.772919 -0.981871 -0.802418  1.175303  0.440616 -0.982076  \n",
       "504 -0.667776 -0.981871 -0.802418  1.175303  0.402826 -0.864446  \n",
       "505 -0.612640 -0.981871 -0.802418  1.175303  0.440616 -0.668397  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e938bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(X_data.values.astype(np.float16).astype(np.float32)) \n",
    "targets = torch.tensor(y_data.values.astype(np.float16).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f86c02a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4194,  0.2847, -1.2871,  ..., -1.4580,  0.4407, -1.0742],\n",
       "        [-0.4170, -0.4873, -0.5928,  ..., -0.3027,  0.4407, -0.4919],\n",
       "        [-0.4170, -0.4873, -0.5928,  ..., -0.3027,  0.3960, -1.2080],\n",
       "        ...,\n",
       "        [-0.4131, -0.4873,  0.1156,  ...,  1.1758,  0.4407, -0.9819],\n",
       "        [-0.4075, -0.4873,  0.1156,  ...,  1.1758,  0.4028, -0.8643],\n",
       "        [-0.4146, -0.4873,  0.1156,  ...,  1.1758,  0.4407, -0.6685]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f40e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments of projects\n",
    "batch_size = 16\n",
    "epochs = 500\n",
    "train_test_split = 0.8\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bfae2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features[:int(len(features)*train_test_split)]\n",
    "train_y = targets[:int(len(features)*train_test_split)]\n",
    "\n",
    "test_x = features[int(len(features)*train_test_split)+1:]\n",
    "test_y = targets[int(len(features)*train_test_split)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b13c89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X,y):\n",
    "    batches = []\n",
    "    \n",
    "    for index in range(0,len(train_x)+1,batch_size):\n",
    "        batches.append((X[index:index+batch_size],y[index:index+batch_size]))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "781448fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches=get_batches(train_x,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c8e37",
   "metadata": {},
   "source": [
    "<h1>Plaintext Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cf48b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a2b9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSyNet(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(LinearSyNet, self).__init__(torch_ref=torch_ref)\n",
    "        self.fc1 = self.torch_ref.nn.Linear(13,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c476e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSyNet(torch)\n",
    "criterion = torch.nn.MSELoss(reduction='mean') \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "428cbbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500  Running Loss : 978.0725708007812 and testacc : 367.93927001953125\n",
      "Epoch 1/500  Running Loss : 831.0911865234375 and testacc : 411.168212890625\n",
      "Epoch 2/500  Running Loss : 720.55322265625 and testacc : 440.28497314453125\n",
      "Epoch 3/500  Running Loss : 633.6892700195312 and testacc : 457.1828308105469\n",
      "Epoch 4/500  Running Loss : 562.962646484375 and testacc : 464.29931640625\n",
      "Epoch 5/500  Running Loss : 503.80291748046875 and testacc : 463.9801330566406\n",
      "Epoch 6/500  Running Loss : 453.3403015136719 and testacc : 458.2357177734375\n",
      "Epoch 7/500  Running Loss : 409.6958312988281 and testacc : 448.6855773925781\n",
      "Epoch 8/500  Running Loss : 371.5797424316406 and testacc : 436.5837707519531\n",
      "Epoch 9/500  Running Loss : 338.0628967285156 and testacc : 422.87548828125\n",
      "Epoch 10/500  Running Loss : 308.4455261230469 and testacc : 408.2576904296875\n",
      "Epoch 11/500  Running Loss : 282.1790466308594 and testacc : 393.2343444824219\n",
      "Epoch 12/500  Running Loss : 258.81988525390625 and testacc : 378.1633605957031\n",
      "Epoch 13/500  Running Loss : 238.00051879882812 and testacc : 363.2930603027344\n",
      "Epoch 14/500  Running Loss : 219.4108123779297 and testacc : 348.7910461425781\n",
      "Epoch 15/500  Running Loss : 202.78579711914062 and testacc : 334.7660827636719\n",
      "Epoch 16/500  Running Loss : 187.89691162109375 and testacc : 321.284912109375\n",
      "Epoch 17/500  Running Loss : 174.5458221435547 and testacc : 308.38336181640625\n",
      "Epoch 18/500  Running Loss : 162.55923461914062 and testacc : 296.076416015625\n",
      "Epoch 19/500  Running Loss : 151.7855987548828 and testacc : 284.3641357421875\n",
      "Epoch 20/500  Running Loss : 142.091552734375 and testacc : 273.23583984375\n",
      "Epoch 21/500  Running Loss : 133.3596649169922 and testacc : 262.6748352050781\n",
      "Epoch 22/500  Running Loss : 125.48639678955078 and testacc : 252.6597900390625\n",
      "Epoch 23/500  Running Loss : 118.38018798828125 and testacc : 243.16697692871094\n",
      "Epoch 24/500  Running Loss : 111.96000671386719 and testacc : 234.17144775390625\n",
      "Epoch 25/500  Running Loss : 106.15399932861328 and testacc : 225.6476593017578\n",
      "Epoch 26/500  Running Loss : 100.89836883544922 and testacc : 217.5703582763672\n",
      "Epoch 27/500  Running Loss : 96.13651275634766 and testacc : 209.9147186279297\n",
      "Epoch 28/500  Running Loss : 91.81795501708984 and testacc : 202.65699768066406\n",
      "Epoch 29/500  Running Loss : 87.89775085449219 and testacc : 195.77415466308594\n",
      "Epoch 30/500  Running Loss : 84.33589172363281 and testacc : 189.2443389892578\n",
      "Epoch 31/500  Running Loss : 81.09656524658203 and testacc : 183.04678344726562\n",
      "Epoch 32/500  Running Loss : 78.14794158935547 and testacc : 177.16195678710938\n",
      "Epoch 33/500  Running Loss : 75.46138763427734 and testacc : 171.57127380371094\n",
      "Epoch 34/500  Running Loss : 73.01140594482422 and testacc : 166.25747680664062\n",
      "Epoch 35/500  Running Loss : 70.77510070800781 and testacc : 161.2041473388672\n",
      "Epoch 36/500  Running Loss : 68.7318344116211 and testacc : 156.39573669433594\n",
      "Epoch 37/500  Running Loss : 66.86328125 and testacc : 151.8181915283203\n",
      "Epoch 38/500  Running Loss : 65.15292358398438 and testacc : 147.45770263671875\n",
      "Epoch 39/500  Running Loss : 63.585838317871094 and testacc : 143.3018035888672\n",
      "Epoch 40/500  Running Loss : 62.14863967895508 and testacc : 139.33837890625\n",
      "Epoch 41/500  Running Loss : 60.829307556152344 and testacc : 135.55654907226562\n",
      "Epoch 42/500  Running Loss : 59.61700439453125 and testacc : 131.94569396972656\n",
      "Epoch 43/500  Running Loss : 58.50190734863281 and testacc : 128.4962615966797\n",
      "Epoch 44/500  Running Loss : 57.47517776489258 and testacc : 125.1988525390625\n",
      "Epoch 45/500  Running Loss : 56.52889633178711 and testacc : 122.04502868652344\n",
      "Epoch 46/500  Running Loss : 55.655853271484375 and testacc : 119.02680969238281\n",
      "Epoch 47/500  Running Loss : 54.84951400756836 and testacc : 116.136474609375\n",
      "Epoch 48/500  Running Loss : 54.103973388671875 and testacc : 113.36707305908203\n",
      "Epoch 49/500  Running Loss : 53.41393280029297 and testacc : 110.7120361328125\n",
      "Epoch 50/500  Running Loss : 52.774539947509766 and testacc : 108.1650619506836\n",
      "Epoch 51/500  Running Loss : 52.18141555786133 and testacc : 105.72029113769531\n",
      "Epoch 52/500  Running Loss : 51.63058853149414 and testacc : 103.37239074707031\n",
      "Epoch 53/500  Running Loss : 51.118465423583984 and testacc : 101.11607360839844\n",
      "Epoch 54/500  Running Loss : 50.64175796508789 and testacc : 98.94660949707031\n",
      "Epoch 55/500  Running Loss : 50.197505950927734 and testacc : 96.85958099365234\n",
      "Epoch 56/500  Running Loss : 49.783016204833984 and testacc : 94.8506088256836\n",
      "Epoch 57/500  Running Loss : 49.39579772949219 and testacc : 92.91569519042969\n",
      "Epoch 58/500  Running Loss : 49.03364562988281 and testacc : 91.05118560791016\n",
      "Epoch 59/500  Running Loss : 48.694522857666016 and testacc : 89.25347137451172\n",
      "Epoch 60/500  Running Loss : 48.37654113769531 and testacc : 87.51924133300781\n",
      "Epoch 61/500  Running Loss : 48.0779914855957 and testacc : 85.84532928466797\n",
      "Epoch 62/500  Running Loss : 47.79737854003906 and testacc : 84.22891235351562\n",
      "Epoch 63/500  Running Loss : 47.53329086303711 and testacc : 82.66725158691406\n",
      "Epoch 64/500  Running Loss : 47.284420013427734 and testacc : 81.15760040283203\n",
      "Epoch 65/500  Running Loss : 47.049598693847656 and testacc : 79.69763946533203\n",
      "Epoch 66/500  Running Loss : 46.827754974365234 and testacc : 78.2849349975586\n",
      "Epoch 67/500  Running Loss : 46.617897033691406 and testacc : 76.91742706298828\n",
      "Epoch 68/500  Running Loss : 46.419151306152344 and testacc : 75.59300231933594\n",
      "Epoch 69/500  Running Loss : 46.230655670166016 and testacc : 74.30975341796875\n",
      "Epoch 70/500  Running Loss : 46.051700592041016 and testacc : 73.06578063964844\n",
      "Epoch 71/500  Running Loss : 45.88155746459961 and testacc : 71.8593978881836\n",
      "Epoch 72/500  Running Loss : 45.719608306884766 and testacc : 70.68897247314453\n",
      "Epoch 73/500  Running Loss : 45.56525802612305 and testacc : 69.55294036865234\n",
      "Epoch 74/500  Running Loss : 45.41799545288086 and testacc : 68.44985961914062\n",
      "Epoch 75/500  Running Loss : 45.27731704711914 and testacc : 67.3782958984375\n",
      "Epoch 76/500  Running Loss : 45.14276885986328 and testacc : 66.3370361328125\n",
      "Epoch 77/500  Running Loss : 45.01395797729492 and testacc : 65.32485961914062\n",
      "Epoch 78/500  Running Loss : 44.890472412109375 and testacc : 64.34046936035156\n",
      "Epoch 79/500  Running Loss : 44.77199172973633 and testacc : 63.38288879394531\n",
      "Epoch 80/500  Running Loss : 44.658180236816406 and testacc : 62.45102310180664\n",
      "Epoch 81/500  Running Loss : 44.54874038696289 and testacc : 61.54381561279297\n",
      "Epoch 82/500  Running Loss : 44.44339370727539 and testacc : 60.660343170166016\n",
      "Epoch 83/500  Running Loss : 44.34188461303711 and testacc : 59.79975128173828\n",
      "Epoch 84/500  Running Loss : 44.243988037109375 and testacc : 58.961185455322266\n",
      "Epoch 85/500  Running Loss : 44.14947509765625 and testacc : 58.14378356933594\n",
      "Epoch 86/500  Running Loss : 44.05816650390625 and testacc : 57.34678649902344\n",
      "Epoch 87/500  Running Loss : 43.96985626220703 and testacc : 56.56957244873047\n",
      "Epoch 88/500  Running Loss : 43.88438415527344 and testacc : 55.81130599975586\n",
      "Epoch 89/500  Running Loss : 43.80158996582031 and testacc : 55.071327209472656\n",
      "Epoch 90/500  Running Loss : 43.7213020324707 and testacc : 54.349029541015625\n",
      "Epoch 91/500  Running Loss : 43.6434326171875 and testacc : 53.64389419555664\n",
      "Epoch 92/500  Running Loss : 43.567813873291016 and testacc : 52.95525360107422\n",
      "Epoch 93/500  Running Loss : 43.494346618652344 and testacc : 52.28257369995117\n",
      "Epoch 94/500  Running Loss : 43.42291259765625 and testacc : 51.625404357910156\n",
      "Epoch 95/500  Running Loss : 43.35342025756836 and testacc : 50.98322296142578\n",
      "Epoch 96/500  Running Loss : 43.285762786865234 and testacc : 50.3554801940918\n",
      "Epoch 97/500  Running Loss : 43.21986770629883 and testacc : 49.74178695678711\n",
      "Epoch 98/500  Running Loss : 43.155635833740234 and testacc : 49.14167022705078\n",
      "Epoch 99/500  Running Loss : 43.09299087524414 and testacc : 48.55470657348633\n",
      "Epoch 100/500  Running Loss : 43.0318603515625 and testacc : 47.98057174682617\n",
      "Epoch 101/500  Running Loss : 42.97218704223633 and testacc : 47.418861389160156\n",
      "Epoch 102/500  Running Loss : 42.91390609741211 and testacc : 46.869140625\n",
      "Epoch 103/500  Running Loss : 42.856937408447266 and testacc : 46.33111572265625\n",
      "Epoch 104/500  Running Loss : 42.80125427246094 and testacc : 45.804500579833984\n",
      "Epoch 105/500  Running Loss : 42.746795654296875 and testacc : 45.28887939453125\n",
      "Epoch 106/500  Running Loss : 42.69351577758789 and testacc : 44.784088134765625\n",
      "Epoch 107/500  Running Loss : 42.641353607177734 and testacc : 44.28965759277344\n",
      "Epoch 108/500  Running Loss : 42.59027862548828 and testacc : 43.80537414550781\n",
      "Epoch 109/500  Running Loss : 42.54023361206055 and testacc : 43.331031799316406\n",
      "Epoch 110/500  Running Loss : 42.491207122802734 and testacc : 42.86627960205078\n",
      "Epoch 111/500  Running Loss : 42.44314956665039 and testacc : 42.410919189453125\n",
      "Epoch 112/500  Running Loss : 42.39602279663086 and testacc : 41.96467590332031\n",
      "Epoch 113/500  Running Loss : 42.34979248046875 and testacc : 41.527347564697266\n",
      "Epoch 114/500  Running Loss : 42.304443359375 and testacc : 41.09870147705078\n",
      "Epoch 115/500  Running Loss : 42.259925842285156 and testacc : 40.67849349975586\n",
      "Epoch 116/500  Running Loss : 42.21622848510742 and testacc : 40.26654815673828\n",
      "Epoch 117/500  Running Loss : 42.17331314086914 and testacc : 39.86261749267578\n",
      "Epoch 118/500  Running Loss : 42.131160736083984 and testacc : 39.466590881347656\n",
      "Epoch 119/500  Running Loss : 42.089759826660156 and testacc : 39.0782356262207\n",
      "Epoch 120/500  Running Loss : 42.04906463623047 and testacc : 38.69734573364258\n",
      "Epoch 121/500  Running Loss : 42.00907516479492 and testacc : 38.323787689208984\n",
      "Epoch 122/500  Running Loss : 41.96976089477539 and testacc : 37.95735549926758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/500  Running Loss : 41.93108367919922 and testacc : 37.597877502441406\n",
      "Epoch 124/500  Running Loss : 41.893070220947266 and testacc : 37.24527359008789\n",
      "Epoch 125/500  Running Loss : 41.85566711425781 and testacc : 36.89934539794922\n",
      "Epoch 126/500  Running Loss : 41.818870544433594 and testacc : 36.5599479675293\n",
      "Epoch 127/500  Running Loss : 41.78266525268555 and testacc : 36.22693634033203\n",
      "Epoch 128/500  Running Loss : 41.74702453613281 and testacc : 35.90016174316406\n",
      "Epoch 129/500  Running Loss : 41.71194839477539 and testacc : 35.579463958740234\n",
      "Epoch 130/500  Running Loss : 41.67741775512695 and testacc : 35.264793395996094\n",
      "Epoch 131/500  Running Loss : 41.64341354370117 and testacc : 34.95596694946289\n",
      "Epoch 132/500  Running Loss : 41.609928131103516 and testacc : 34.652870178222656\n",
      "Epoch 133/500  Running Loss : 41.576942443847656 and testacc : 34.35540008544922\n",
      "Epoch 134/500  Running Loss : 41.544464111328125 and testacc : 34.063453674316406\n",
      "Epoch 135/500  Running Loss : 41.5124626159668 and testacc : 33.776878356933594\n",
      "Epoch 136/500  Running Loss : 41.48093032836914 and testacc : 33.49558639526367\n",
      "Epoch 137/500  Running Loss : 41.44984436035156 and testacc : 33.2194938659668\n",
      "Epoch 138/500  Running Loss : 41.419227600097656 and testacc : 32.948429107666016\n",
      "Epoch 139/500  Running Loss : 41.3890380859375 and testacc : 32.68235778808594\n",
      "Epoch 140/500  Running Loss : 41.35927963256836 and testacc : 32.4211540222168\n",
      "Epoch 141/500  Running Loss : 41.32994079589844 and testacc : 32.164710998535156\n",
      "Epoch 142/500  Running Loss : 41.301021575927734 and testacc : 31.912988662719727\n",
      "Epoch 143/500  Running Loss : 41.27251052856445 and testacc : 31.66588592529297\n",
      "Epoch 144/500  Running Loss : 41.244384765625 and testacc : 31.42328453063965\n",
      "Epoch 145/500  Running Loss : 41.21664810180664 and testacc : 31.185121536254883\n",
      "Epoch 146/500  Running Loss : 41.18929672241211 and testacc : 30.95130157470703\n",
      "Epoch 147/500  Running Loss : 41.16231155395508 and testacc : 30.721752166748047\n",
      "Epoch 148/500  Running Loss : 41.13569641113281 and testacc : 30.496389389038086\n",
      "Epoch 149/500  Running Loss : 41.10943603515625 and testacc : 30.275148391723633\n",
      "Epoch 150/500  Running Loss : 41.083518981933594 and testacc : 30.05791473388672\n",
      "Epoch 151/500  Running Loss : 41.05794906616211 and testacc : 29.844669342041016\n",
      "Epoch 152/500  Running Loss : 41.03272247314453 and testacc : 29.635345458984375\n",
      "Epoch 153/500  Running Loss : 41.007835388183594 and testacc : 29.4298038482666\n",
      "Epoch 154/500  Running Loss : 40.98326110839844 and testacc : 29.228036880493164\n",
      "Epoch 155/500  Running Loss : 40.95900344848633 and testacc : 29.029956817626953\n",
      "Epoch 156/500  Running Loss : 40.9350700378418 and testacc : 28.83550262451172\n",
      "Epoch 157/500  Running Loss : 40.911434173583984 and testacc : 28.644594192504883\n",
      "Epoch 158/500  Running Loss : 40.88811492919922 and testacc : 28.45717430114746\n",
      "Epoch 159/500  Running Loss : 40.865074157714844 and testacc : 28.27318572998047\n",
      "Epoch 160/500  Running Loss : 40.842342376708984 and testacc : 28.09259033203125\n",
      "Epoch 161/500  Running Loss : 40.819881439208984 and testacc : 27.91530418395996\n",
      "Epoch 162/500  Running Loss : 40.7977180480957 and testacc : 27.741268157958984\n",
      "Epoch 163/500  Running Loss : 40.775821685791016 and testacc : 27.570449829101562\n",
      "Epoch 164/500  Running Loss : 40.75420379638672 and testacc : 27.40277862548828\n",
      "Epoch 165/500  Running Loss : 40.732845306396484 and testacc : 27.23822593688965\n",
      "Epoch 166/500  Running Loss : 40.71176528930664 and testacc : 27.076698303222656\n",
      "Epoch 167/500  Running Loss : 40.69093704223633 and testacc : 26.91815757751465\n",
      "Epoch 168/500  Running Loss : 40.670372009277344 and testacc : 26.762569427490234\n",
      "Epoch 169/500  Running Loss : 40.650047302246094 and testacc : 26.609878540039062\n",
      "Epoch 170/500  Running Loss : 40.629974365234375 and testacc : 26.460018157958984\n",
      "Epoch 171/500  Running Loss : 40.61014175415039 and testacc : 26.312973022460938\n",
      "Epoch 172/500  Running Loss : 40.59054946899414 and testacc : 26.16868019104004\n",
      "Epoch 173/500  Running Loss : 40.57119369506836 and testacc : 26.027069091796875\n",
      "Epoch 174/500  Running Loss : 40.552066802978516 and testacc : 25.888141632080078\n",
      "Epoch 175/500  Running Loss : 40.533172607421875 and testacc : 25.751798629760742\n",
      "Epoch 176/500  Running Loss : 40.514495849609375 and testacc : 25.61804962158203\n",
      "Epoch 177/500  Running Loss : 40.49604797363281 and testacc : 25.486841201782227\n",
      "Epoch 178/500  Running Loss : 40.47781753540039 and testacc : 25.358123779296875\n",
      "Epoch 179/500  Running Loss : 40.45980453491211 and testacc : 25.23185157775879\n",
      "Epoch 180/500  Running Loss : 40.44198989868164 and testacc : 25.107990264892578\n",
      "Epoch 181/500  Running Loss : 40.424400329589844 and testacc : 24.98651123046875\n",
      "Epoch 182/500  Running Loss : 40.407005310058594 and testacc : 24.867368698120117\n",
      "Epoch 183/500  Running Loss : 40.38981628417969 and testacc : 24.750503540039062\n",
      "Epoch 184/500  Running Loss : 40.372825622558594 and testacc : 24.635879516601562\n",
      "Epoch 185/500  Running Loss : 40.35603713989258 and testacc : 24.52349281311035\n",
      "Epoch 186/500  Running Loss : 40.33943557739258 and testacc : 24.413293838500977\n",
      "Epoch 187/500  Running Loss : 40.323020935058594 and testacc : 24.305246353149414\n",
      "Epoch 188/500  Running Loss : 40.30680847167969 and testacc : 24.199321746826172\n",
      "Epoch 189/500  Running Loss : 40.290767669677734 and testacc : 24.095460891723633\n",
      "Epoch 190/500  Running Loss : 40.27491760253906 and testacc : 23.993669509887695\n",
      "Epoch 191/500  Running Loss : 40.259246826171875 and testacc : 23.893898010253906\n",
      "Epoch 192/500  Running Loss : 40.24375534057617 and testacc : 23.796104431152344\n",
      "Epoch 193/500  Running Loss : 40.228431701660156 and testacc : 23.700260162353516\n",
      "Epoch 194/500  Running Loss : 40.21328353881836 and testacc : 23.606338500976562\n",
      "Epoch 195/500  Running Loss : 40.19831466674805 and testacc : 23.514305114746094\n",
      "Epoch 196/500  Running Loss : 40.18351364135742 and testacc : 23.424110412597656\n",
      "Epoch 197/500  Running Loss : 40.16886901855469 and testacc : 23.33576774597168\n",
      "Epoch 198/500  Running Loss : 40.154388427734375 and testacc : 23.24924087524414\n",
      "Epoch 199/500  Running Loss : 40.140071868896484 and testacc : 23.164461135864258\n",
      "Epoch 200/500  Running Loss : 40.125911712646484 and testacc : 23.08142852783203\n",
      "Epoch 201/500  Running Loss : 40.11191177368164 and testacc : 23.000110626220703\n",
      "Epoch 202/500  Running Loss : 40.09806442260742 and testacc : 22.920501708984375\n",
      "Epoch 203/500  Running Loss : 40.084373474121094 and testacc : 22.84253692626953\n",
      "Epoch 204/500  Running Loss : 40.07083511352539 and testacc : 22.766212463378906\n",
      "Epoch 205/500  Running Loss : 40.057437896728516 and testacc : 22.69149398803711\n",
      "Epoch 206/500  Running Loss : 40.044185638427734 and testacc : 22.618364334106445\n",
      "Epoch 207/500  Running Loss : 40.031089782714844 and testacc : 22.546810150146484\n",
      "Epoch 208/500  Running Loss : 40.01811599731445 and testacc : 22.476770401000977\n",
      "Epoch 209/500  Running Loss : 40.00531005859375 and testacc : 22.40825843811035\n",
      "Epoch 210/500  Running Loss : 39.99263381958008 and testacc : 22.341228485107422\n",
      "Epoch 211/500  Running Loss : 39.9800910949707 and testacc : 22.275659561157227\n",
      "Epoch 212/500  Running Loss : 39.967674255371094 and testacc : 22.211524963378906\n",
      "Epoch 213/500  Running Loss : 39.95540237426758 and testacc : 22.14882469177246\n",
      "Epoch 214/500  Running Loss : 39.943267822265625 and testacc : 22.087520599365234\n",
      "Epoch 215/500  Running Loss : 39.93125915527344 and testacc : 22.0275821685791\n",
      "Epoch 216/500  Running Loss : 39.91938018798828 and testacc : 21.968982696533203\n",
      "Epoch 217/500  Running Loss : 39.90762710571289 and testacc : 21.911720275878906\n",
      "Epoch 218/500  Running Loss : 39.89599609375 and testacc : 21.855758666992188\n",
      "Epoch 219/500  Running Loss : 39.884490966796875 and testacc : 21.80109214782715\n",
      "Epoch 220/500  Running Loss : 39.87311553955078 and testacc : 21.747699737548828\n",
      "Epoch 221/500  Running Loss : 39.861854553222656 and testacc : 21.6955623626709\n",
      "Epoch 222/500  Running Loss : 39.850711822509766 and testacc : 21.644638061523438\n",
      "Epoch 223/500  Running Loss : 39.83968734741211 and testacc : 21.594924926757812\n",
      "Epoch 224/500  Running Loss : 39.82878112792969 and testacc : 21.546396255493164\n",
      "Epoch 225/500  Running Loss : 39.817996978759766 and testacc : 21.49904441833496\n",
      "Epoch 226/500  Running Loss : 39.80730438232422 and testacc : 21.45285415649414\n",
      "Epoch 227/500  Running Loss : 39.79674530029297 and testacc : 21.407787322998047\n",
      "Epoch 228/500  Running Loss : 39.786285400390625 and testacc : 21.363840103149414\n",
      "Epoch 229/500  Running Loss : 39.77593231201172 and testacc : 21.32098960876465\n",
      "Epoch 230/500  Running Loss : 39.76569366455078 and testacc : 21.279216766357422\n",
      "Epoch 231/500  Running Loss : 39.75556182861328 and testacc : 21.238508224487305\n",
      "Epoch 232/500  Running Loss : 39.74553298950195 and testacc : 21.19884490966797\n",
      "Epoch 233/500  Running Loss : 39.73561477661133 and testacc : 21.16020965576172\n",
      "Epoch 234/500  Running Loss : 39.725799560546875 and testacc : 21.122589111328125\n",
      "Epoch 235/500  Running Loss : 39.7160758972168 and testacc : 21.08596420288086\n",
      "Epoch 236/500  Running Loss : 39.70645523071289 and testacc : 21.050317764282227\n",
      "Epoch 237/500  Running Loss : 39.696937561035156 and testacc : 21.015642166137695\n",
      "Epoch 238/500  Running Loss : 39.6875114440918 and testacc : 20.981918334960938\n",
      "Epoch 239/500  Running Loss : 39.67819595336914 and testacc : 20.949127197265625\n",
      "Epoch 240/500  Running Loss : 39.66896057128906 and testacc : 20.917255401611328\n",
      "Epoch 241/500  Running Loss : 39.659828186035156 and testacc : 20.88629150390625\n",
      "Epoch 242/500  Running Loss : 39.65079116821289 and testacc : 20.85620880126953\n",
      "Epoch 243/500  Running Loss : 39.6418342590332 and testacc : 20.826997756958008\n",
      "Epoch 244/500  Running Loss : 39.632972717285156 and testacc : 20.798662185668945\n",
      "Epoch 245/500  Running Loss : 39.62420654296875 and testacc : 20.771162033081055\n",
      "Epoch 246/500  Running Loss : 39.61553192138672 and testacc : 20.744508743286133\n",
      "Epoch 247/500  Running Loss : 39.60693359375 and testacc : 20.71866798400879\n",
      "Epoch 248/500  Running Loss : 39.59843063354492 and testacc : 20.69363021850586\n",
      "Epoch 249/500  Running Loss : 39.59001159667969 and testacc : 20.669389724731445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/500  Running Loss : 39.58168411254883 and testacc : 20.645917892456055\n",
      "Epoch 251/500  Running Loss : 39.573429107666016 and testacc : 20.62322998046875\n",
      "Epoch 252/500  Running Loss : 39.56526565551758 and testacc : 20.601301193237305\n",
      "Epoch 253/500  Running Loss : 39.55717086791992 and testacc : 20.580106735229492\n",
      "Epoch 254/500  Running Loss : 39.54916763305664 and testacc : 20.559650421142578\n",
      "Epoch 255/500  Running Loss : 39.54125213623047 and testacc : 20.53989601135254\n",
      "Epoch 256/500  Running Loss : 39.53340148925781 and testacc : 20.52086639404297\n",
      "Epoch 257/500  Running Loss : 39.525630950927734 and testacc : 20.502532958984375\n",
      "Epoch 258/500  Running Loss : 39.5179443359375 and testacc : 20.484880447387695\n",
      "Epoch 259/500  Running Loss : 39.51033020019531 and testacc : 20.467897415161133\n",
      "Epoch 260/500  Running Loss : 39.50278854370117 and testacc : 20.45157814025879\n",
      "Epoch 261/500  Running Loss : 39.495323181152344 and testacc : 20.4359073638916\n",
      "Epoch 262/500  Running Loss : 39.48793411254883 and testacc : 20.420883178710938\n",
      "Epoch 263/500  Running Loss : 39.48061752319336 and testacc : 20.40648651123047\n",
      "Epoch 264/500  Running Loss : 39.473365783691406 and testacc : 20.3927059173584\n",
      "Epoch 265/500  Running Loss : 39.46619415283203 and testacc : 20.379535675048828\n",
      "Epoch 266/500  Running Loss : 39.45909118652344 and testacc : 20.366954803466797\n",
      "Epoch 267/500  Running Loss : 39.45205307006836 and testacc : 20.3549747467041\n",
      "Epoch 268/500  Running Loss : 39.445091247558594 and testacc : 20.343563079833984\n",
      "Epoch 269/500  Running Loss : 39.43818664550781 and testacc : 20.332725524902344\n",
      "Epoch 270/500  Running Loss : 39.43136215209961 and testacc : 20.322439193725586\n",
      "Epoch 271/500  Running Loss : 39.424591064453125 and testacc : 20.31270980834961\n",
      "Epoch 272/500  Running Loss : 39.41788864135742 and testacc : 20.303518295288086\n",
      "Epoch 273/500  Running Loss : 39.41126251220703 and testacc : 20.294851303100586\n",
      "Epoch 274/500  Running Loss : 39.404685974121094 and testacc : 20.286712646484375\n",
      "Epoch 275/500  Running Loss : 39.398189544677734 and testacc : 20.279090881347656\n",
      "Epoch 276/500  Running Loss : 39.391727447509766 and testacc : 20.271970748901367\n",
      "Epoch 277/500  Running Loss : 39.38535690307617 and testacc : 20.265352249145508\n",
      "Epoch 278/500  Running Loss : 39.37903594970703 and testacc : 20.259214401245117\n",
      "Epoch 279/500  Running Loss : 39.372772216796875 and testacc : 20.253557205200195\n",
      "Epoch 280/500  Running Loss : 39.366573333740234 and testacc : 20.248369216918945\n",
      "Epoch 281/500  Running Loss : 39.360435485839844 and testacc : 20.243650436401367\n",
      "Epoch 282/500  Running Loss : 39.354347229003906 and testacc : 20.239383697509766\n",
      "Epoch 283/500  Running Loss : 39.348331451416016 and testacc : 20.23556137084961\n",
      "Epoch 284/500  Running Loss : 39.34236145019531 and testacc : 20.2321720123291\n",
      "Epoch 285/500  Running Loss : 39.336456298828125 and testacc : 20.229219436645508\n",
      "Epoch 286/500  Running Loss : 39.330596923828125 and testacc : 20.22669219970703\n",
      "Epoch 287/500  Running Loss : 39.32480239868164 and testacc : 20.224578857421875\n",
      "Epoch 288/500  Running Loss : 39.31906509399414 and testacc : 20.222871780395508\n",
      "Epoch 289/500  Running Loss : 39.31336975097656 and testacc : 20.221569061279297\n",
      "Epoch 290/500  Running Loss : 39.307735443115234 and testacc : 20.220651626586914\n",
      "Epoch 291/500  Running Loss : 39.30215835571289 and testacc : 20.220134735107422\n",
      "Epoch 292/500  Running Loss : 39.29661560058594 and testacc : 20.219989776611328\n",
      "Epoch 293/500  Running Loss : 39.29114532470703 and testacc : 20.22022247314453\n",
      "Epoch 294/500  Running Loss : 39.285709381103516 and testacc : 20.220821380615234\n",
      "Epoch 295/500  Running Loss : 39.28034210205078 and testacc : 20.22177505493164\n",
      "Epoch 296/500  Running Loss : 39.2750129699707 and testacc : 20.22309112548828\n",
      "Epoch 297/500  Running Loss : 39.269737243652344 and testacc : 20.224748611450195\n",
      "Epoch 298/500  Running Loss : 39.2645149230957 and testacc : 20.22675132751465\n",
      "Epoch 299/500  Running Loss : 39.25933837890625 and testacc : 20.229074478149414\n",
      "Epoch 300/500  Running Loss : 39.25419998168945 and testacc : 20.23174476623535\n",
      "Epoch 301/500  Running Loss : 39.24912643432617 and testacc : 20.23473358154297\n",
      "Epoch 302/500  Running Loss : 39.244083404541016 and testacc : 20.23802947998047\n",
      "Epoch 303/500  Running Loss : 39.23908996582031 and testacc : 20.24164581298828\n",
      "Epoch 304/500  Running Loss : 39.23415756225586 and testacc : 20.24556541442871\n",
      "Epoch 305/500  Running Loss : 39.229251861572266 and testacc : 20.249784469604492\n",
      "Epoch 306/500  Running Loss : 39.22439956665039 and testacc : 20.25429344177246\n",
      "Epoch 307/500  Running Loss : 39.21958923339844 and testacc : 20.25909423828125\n",
      "Epoch 308/500  Running Loss : 39.21483612060547 and testacc : 20.264179229736328\n",
      "Epoch 309/500  Running Loss : 39.210105895996094 and testacc : 20.269533157348633\n",
      "Epoch 310/500  Running Loss : 39.20543670654297 and testacc : 20.27516746520996\n",
      "Epoch 311/500  Running Loss : 39.200801849365234 and testacc : 20.28106689453125\n",
      "Epoch 312/500  Running Loss : 39.19620132446289 and testacc : 20.28721809387207\n",
      "Epoch 313/500  Running Loss : 39.191654205322266 and testacc : 20.293636322021484\n",
      "Epoch 314/500  Running Loss : 39.18714141845703 and testacc : 20.300304412841797\n",
      "Epoch 315/500  Running Loss : 39.18266677856445 and testacc : 20.307220458984375\n",
      "Epoch 316/500  Running Loss : 39.17824935913086 and testacc : 20.314380645751953\n",
      "Epoch 317/500  Running Loss : 39.173858642578125 and testacc : 20.3217716217041\n",
      "Epoch 318/500  Running Loss : 39.16950607299805 and testacc : 20.32940101623535\n",
      "Epoch 319/500  Running Loss : 39.165199279785156 and testacc : 20.337257385253906\n",
      "Epoch 320/500  Running Loss : 39.160919189453125 and testacc : 20.34532928466797\n",
      "Epoch 321/500  Running Loss : 39.15670394897461 and testacc : 20.35363006591797\n",
      "Epoch 322/500  Running Loss : 39.15250015258789 and testacc : 20.36213493347168\n",
      "Epoch 323/500  Running Loss : 39.148345947265625 and testacc : 20.3708553314209\n",
      "Epoch 324/500  Running Loss : 39.144222259521484 and testacc : 20.379789352416992\n",
      "Epoch 325/500  Running Loss : 39.140140533447266 and testacc : 20.388916015625\n",
      "Epoch 326/500  Running Loss : 39.13609313964844 and testacc : 20.398235321044922\n",
      "Epoch 327/500  Running Loss : 39.132080078125 and testacc : 20.407751083374023\n",
      "Epoch 328/500  Running Loss : 39.128108978271484 and testacc : 20.417455673217773\n",
      "Epoch 329/500  Running Loss : 39.12416458129883 and testacc : 20.427345275878906\n",
      "Epoch 330/500  Running Loss : 39.120262145996094 and testacc : 20.43741798400879\n",
      "Epoch 331/500  Running Loss : 39.11638641357422 and testacc : 20.447662353515625\n",
      "Epoch 332/500  Running Loss : 39.112548828125 and testacc : 20.458087921142578\n",
      "Epoch 333/500  Running Loss : 39.10874557495117 and testacc : 20.46868324279785\n",
      "Epoch 334/500  Running Loss : 39.104976654052734 and testacc : 20.47944450378418\n",
      "Epoch 335/500  Running Loss : 39.10124206542969 and testacc : 20.490375518798828\n",
      "Epoch 336/500  Running Loss : 39.09754180908203 and testacc : 20.501455307006836\n",
      "Epoch 337/500  Running Loss : 39.09386444091797 and testacc : 20.5126895904541\n",
      "Epoch 338/500  Running Loss : 39.090232849121094 and testacc : 20.524080276489258\n",
      "Epoch 339/500  Running Loss : 39.08661651611328 and testacc : 20.535627365112305\n",
      "Epoch 340/500  Running Loss : 39.08304977416992 and testacc : 20.547306060791016\n",
      "Epoch 341/500  Running Loss : 39.07950210571289 and testacc : 20.55913543701172\n",
      "Epoch 342/500  Running Loss : 39.07598114013672 and testacc : 20.57109832763672\n",
      "Epoch 343/500  Running Loss : 39.0724983215332 and testacc : 20.583200454711914\n",
      "Epoch 344/500  Running Loss : 39.06904220581055 and testacc : 20.595434188842773\n",
      "Epoch 345/500  Running Loss : 39.06562042236328 and testacc : 20.607799530029297\n",
      "Epoch 346/500  Running Loss : 39.06222915649414 and testacc : 20.620298385620117\n",
      "Epoch 347/500  Running Loss : 39.05885314941406 and testacc : 20.632902145385742\n",
      "Epoch 348/500  Running Loss : 39.05553436279297 and testacc : 20.64563751220703\n",
      "Epoch 349/500  Running Loss : 39.05221939086914 and testacc : 20.658483505249023\n",
      "Epoch 350/500  Running Loss : 39.0489387512207 and testacc : 20.67145347595215\n",
      "Epoch 351/500  Running Loss : 39.04568862915039 and testacc : 20.684532165527344\n",
      "Epoch 352/500  Running Loss : 39.0424690246582 and testacc : 20.69772720336914\n",
      "Epoch 353/500  Running Loss : 39.03926467895508 and testacc : 20.71101951599121\n",
      "Epoch 354/500  Running Loss : 39.03609848022461 and testacc : 20.724411010742188\n",
      "Epoch 355/500  Running Loss : 39.032955169677734 and testacc : 20.73789405822754\n",
      "Epoch 356/500  Running Loss : 39.02984619140625 and testacc : 20.751489639282227\n",
      "Epoch 357/500  Running Loss : 39.02676010131836 and testacc : 20.765193939208984\n",
      "Epoch 358/500  Running Loss : 39.023685455322266 and testacc : 20.778980255126953\n",
      "Epoch 359/500  Running Loss : 39.020652770996094 and testacc : 20.792858123779297\n",
      "Epoch 360/500  Running Loss : 39.01765060424805 and testacc : 20.80681037902832\n",
      "Epoch 361/500  Running Loss : 39.01465606689453 and testacc : 20.820865631103516\n",
      "Epoch 362/500  Running Loss : 39.011695861816406 and testacc : 20.83498191833496\n",
      "Epoch 363/500  Running Loss : 39.008766174316406 and testacc : 20.84920883178711\n",
      "Epoch 364/500  Running Loss : 39.005859375 and testacc : 20.863489151000977\n",
      "Epoch 365/500  Running Loss : 39.00296401977539 and testacc : 20.877857208251953\n",
      "Epoch 366/500  Running Loss : 39.000099182128906 and testacc : 20.89231300354004\n",
      "Epoch 367/500  Running Loss : 38.99726867675781 and testacc : 20.906829833984375\n",
      "Epoch 368/500  Running Loss : 38.99444580078125 and testacc : 20.921415328979492\n",
      "Epoch 369/500  Running Loss : 38.991661071777344 and testacc : 20.936071395874023\n",
      "Epoch 370/500  Running Loss : 38.9888916015625 and testacc : 20.950786590576172\n",
      "Epoch 371/500  Running Loss : 38.986148834228516 and testacc : 20.965574264526367\n",
      "Epoch 372/500  Running Loss : 38.983421325683594 and testacc : 20.98041534423828\n",
      "Epoch 373/500  Running Loss : 38.98072052001953 and testacc : 20.99534034729004\n",
      "Epoch 374/500  Running Loss : 38.97804260253906 and testacc : 21.01030731201172\n",
      "Epoch 375/500  Running Loss : 38.97539138793945 and testacc : 21.025327682495117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376/500  Running Loss : 38.97275161743164 and testacc : 21.0404109954834\n",
      "Epoch 377/500  Running Loss : 38.970130920410156 and testacc : 21.05553436279297\n",
      "Epoch 378/500  Running Loss : 38.9675407409668 and testacc : 21.070709228515625\n",
      "Epoch 379/500  Running Loss : 38.9649772644043 and testacc : 21.0859375\n",
      "Epoch 380/500  Running Loss : 38.96242904663086 and testacc : 21.10120964050293\n",
      "Epoch 381/500  Running Loss : 38.959896087646484 and testacc : 21.116533279418945\n",
      "Epoch 382/500  Running Loss : 38.9573860168457 and testacc : 21.131898880004883\n",
      "Epoch 383/500  Running Loss : 38.95490646362305 and testacc : 21.14729881286621\n",
      "Epoch 384/500  Running Loss : 38.95243453979492 and testacc : 21.162734985351562\n",
      "Epoch 385/500  Running Loss : 38.94998550415039 and testacc : 21.178211212158203\n",
      "Epoch 386/500  Running Loss : 38.94756317138672 and testacc : 21.193729400634766\n",
      "Epoch 387/500  Running Loss : 38.945152282714844 and testacc : 21.209272384643555\n",
      "Epoch 388/500  Running Loss : 38.94276809692383 and testacc : 21.2248477935791\n",
      "Epoch 389/500  Running Loss : 38.940399169921875 and testacc : 21.240463256835938\n",
      "Epoch 390/500  Running Loss : 38.938053131103516 and testacc : 21.256101608276367\n",
      "Epoch 391/500  Running Loss : 38.935726165771484 and testacc : 21.271774291992188\n",
      "Epoch 392/500  Running Loss : 38.93341064453125 and testacc : 21.28748321533203\n",
      "Epoch 393/500  Running Loss : 38.931114196777344 and testacc : 21.303213119506836\n",
      "Epoch 394/500  Running Loss : 38.92884063720703 and testacc : 21.318967819213867\n",
      "Epoch 395/500  Running Loss : 38.92658233642578 and testacc : 21.334760665893555\n",
      "Epoch 396/500  Running Loss : 38.924339294433594 and testacc : 21.350555419921875\n",
      "Epoch 397/500  Running Loss : 38.922122955322266 and testacc : 21.366374969482422\n",
      "Epoch 398/500  Running Loss : 38.919918060302734 and testacc : 21.382217407226562\n",
      "Epoch 399/500  Running Loss : 38.917728424072266 and testacc : 21.398086547851562\n",
      "Epoch 400/500  Running Loss : 38.91556167602539 and testacc : 21.413959503173828\n",
      "Epoch 401/500  Running Loss : 38.913414001464844 and testacc : 21.429841995239258\n",
      "Epoch 402/500  Running Loss : 38.911277770996094 and testacc : 21.44574737548828\n",
      "Epoch 403/500  Running Loss : 38.909156799316406 and testacc : 21.461660385131836\n",
      "Epoch 404/500  Running Loss : 38.90705871582031 and testacc : 21.477609634399414\n",
      "Epoch 405/500  Running Loss : 38.90497589111328 and testacc : 21.49355125427246\n",
      "Epoch 406/500  Running Loss : 38.90290069580078 and testacc : 21.509506225585938\n",
      "Epoch 407/500  Running Loss : 38.900856018066406 and testacc : 21.52546501159668\n",
      "Epoch 408/500  Running Loss : 38.89881896972656 and testacc : 21.54142951965332\n",
      "Epoch 409/500  Running Loss : 38.896793365478516 and testacc : 21.557397842407227\n",
      "Epoch 410/500  Running Loss : 38.89479064941406 and testacc : 21.573379516601562\n",
      "Epoch 411/500  Running Loss : 38.89280319213867 and testacc : 21.5893611907959\n",
      "Epoch 412/500  Running Loss : 38.890830993652344 and testacc : 21.60536766052246\n",
      "Epoch 413/500  Running Loss : 38.88886642456055 and testacc : 21.621353149414062\n",
      "Epoch 414/500  Running Loss : 38.88692092895508 and testacc : 21.637344360351562\n",
      "Epoch 415/500  Running Loss : 38.885005950927734 and testacc : 21.653339385986328\n",
      "Epoch 416/500  Running Loss : 38.88309097290039 and testacc : 21.6693172454834\n",
      "Epoch 417/500  Running Loss : 38.881195068359375 and testacc : 21.685321807861328\n",
      "Epoch 418/500  Running Loss : 38.87931442260742 and testacc : 21.70130729675293\n",
      "Epoch 419/500  Running Loss : 38.877437591552734 and testacc : 21.717300415039062\n",
      "Epoch 420/500  Running Loss : 38.875587463378906 and testacc : 21.7332706451416\n",
      "Epoch 421/500  Running Loss : 38.873748779296875 and testacc : 21.74924659729004\n",
      "Epoch 422/500  Running Loss : 38.871917724609375 and testacc : 21.765233993530273\n",
      "Epoch 423/500  Running Loss : 38.8701057434082 and testacc : 21.781192779541016\n",
      "Epoch 424/500  Running Loss : 38.868309020996094 and testacc : 21.797147750854492\n",
      "Epoch 425/500  Running Loss : 38.86652755737305 and testacc : 21.813098907470703\n",
      "Epoch 426/500  Running Loss : 38.86475372314453 and testacc : 21.829029083251953\n",
      "Epoch 427/500  Running Loss : 38.863006591796875 and testacc : 21.84494972229004\n",
      "Epoch 428/500  Running Loss : 38.86125183105469 and testacc : 21.860876083374023\n",
      "Epoch 429/500  Running Loss : 38.859527587890625 and testacc : 21.876792907714844\n",
      "Epoch 430/500  Running Loss : 38.85780334472656 and testacc : 21.892681121826172\n",
      "Epoch 431/500  Running Loss : 38.856101989746094 and testacc : 21.90856170654297\n",
      "Epoch 432/500  Running Loss : 38.85441207885742 and testacc : 21.924440383911133\n",
      "Epoch 433/500  Running Loss : 38.85272979736328 and testacc : 21.94029426574707\n",
      "Epoch 434/500  Running Loss : 38.85105895996094 and testacc : 21.956125259399414\n",
      "Epoch 435/500  Running Loss : 38.84941101074219 and testacc : 21.971942901611328\n",
      "Epoch 436/500  Running Loss : 38.8477668762207 and testacc : 21.987751007080078\n",
      "Epoch 437/500  Running Loss : 38.84613800048828 and testacc : 22.00353240966797\n",
      "Epoch 438/500  Running Loss : 38.844520568847656 and testacc : 22.019304275512695\n",
      "Epoch 439/500  Running Loss : 38.842918395996094 and testacc : 22.035057067871094\n",
      "Epoch 440/500  Running Loss : 38.84132766723633 and testacc : 22.050796508789062\n",
      "Epoch 441/500  Running Loss : 38.839752197265625 and testacc : 22.06649398803711\n",
      "Epoch 442/500  Running Loss : 38.83818054199219 and testacc : 22.082195281982422\n",
      "Epoch 443/500  Running Loss : 38.83662414550781 and testacc : 22.097848892211914\n",
      "Epoch 444/500  Running Loss : 38.8350715637207 and testacc : 22.1135196685791\n",
      "Epoch 445/500  Running Loss : 38.83354187011719 and testacc : 22.129133224487305\n",
      "Epoch 446/500  Running Loss : 38.8320198059082 and testacc : 22.144758224487305\n",
      "Epoch 447/500  Running Loss : 38.83050537109375 and testacc : 22.160348892211914\n",
      "Epoch 448/500  Running Loss : 38.829010009765625 and testacc : 22.17591094970703\n",
      "Epoch 449/500  Running Loss : 38.82752227783203 and testacc : 22.191442489624023\n",
      "Epoch 450/500  Running Loss : 38.82603454589844 and testacc : 22.20697021484375\n",
      "Epoch 451/500  Running Loss : 38.824581146240234 and testacc : 22.222454071044922\n",
      "Epoch 452/500  Running Loss : 38.823116302490234 and testacc : 22.237926483154297\n",
      "Epoch 453/500  Running Loss : 38.8216667175293 and testacc : 22.253389358520508\n",
      "Epoch 454/500  Running Loss : 38.82023239135742 and testacc : 22.268815994262695\n",
      "Epoch 455/500  Running Loss : 38.818809509277344 and testacc : 22.284194946289062\n",
      "Epoch 456/500  Running Loss : 38.8173942565918 and testacc : 22.299558639526367\n",
      "Epoch 457/500  Running Loss : 38.81599044799805 and testacc : 22.314889907836914\n",
      "Epoch 458/500  Running Loss : 38.81459426879883 and testacc : 22.3301944732666\n",
      "Epoch 459/500  Running Loss : 38.813209533691406 and testacc : 22.345476150512695\n",
      "Epoch 460/500  Running Loss : 38.81184005737305 and testacc : 22.36073875427246\n",
      "Epoch 461/500  Running Loss : 38.81047439575195 and testacc : 22.375946044921875\n",
      "Epoch 462/500  Running Loss : 38.809120178222656 and testacc : 22.39113426208496\n",
      "Epoch 463/500  Running Loss : 38.80777359008789 and testacc : 22.406280517578125\n",
      "Epoch 464/500  Running Loss : 38.80644607543945 and testacc : 22.421424865722656\n",
      "Epoch 465/500  Running Loss : 38.80511474609375 and testacc : 22.43653106689453\n",
      "Epoch 466/500  Running Loss : 38.80379867553711 and testacc : 22.451595306396484\n",
      "Epoch 467/500  Running Loss : 38.80250549316406 and testacc : 22.466636657714844\n",
      "Epoch 468/500  Running Loss : 38.80119705200195 and testacc : 22.481653213500977\n",
      "Epoch 469/500  Running Loss : 38.79991912841797 and testacc : 22.49663543701172\n",
      "Epoch 470/500  Running Loss : 38.79864501953125 and testacc : 22.51158332824707\n",
      "Epoch 471/500  Running Loss : 38.79736328125 and testacc : 22.526504516601562\n",
      "Epoch 472/500  Running Loss : 38.796104431152344 and testacc : 22.541378021240234\n",
      "Epoch 473/500  Running Loss : 38.79486083984375 and testacc : 22.556224822998047\n",
      "Epoch 474/500  Running Loss : 38.793609619140625 and testacc : 22.571043014526367\n",
      "Epoch 475/500  Running Loss : 38.792381286621094 and testacc : 22.58580780029297\n",
      "Epoch 476/500  Running Loss : 38.79115295410156 and testacc : 22.600570678710938\n",
      "Epoch 477/500  Running Loss : 38.78994369506836 and testacc : 22.615272521972656\n",
      "Epoch 478/500  Running Loss : 38.78873825073242 and testacc : 22.62995719909668\n",
      "Epoch 479/500  Running Loss : 38.78753662109375 and testacc : 22.644622802734375\n",
      "Epoch 480/500  Running Loss : 38.78635025024414 and testacc : 22.65923500061035\n",
      "Epoch 481/500  Running Loss : 38.78517150878906 and testacc : 22.673810958862305\n",
      "Epoch 482/500  Running Loss : 38.78398895263672 and testacc : 22.688365936279297\n",
      "Epoch 483/500  Running Loss : 38.7828254699707 and testacc : 22.702863693237305\n",
      "Epoch 484/500  Running Loss : 38.78167724609375 and testacc : 22.71734046936035\n",
      "Epoch 485/500  Running Loss : 38.780517578125 and testacc : 22.731788635253906\n",
      "Epoch 486/500  Running Loss : 38.779380798339844 and testacc : 22.746173858642578\n",
      "Epoch 487/500  Running Loss : 38.77825164794922 and testacc : 22.760513305664062\n",
      "Epoch 488/500  Running Loss : 38.77713394165039 and testacc : 22.774856567382812\n",
      "Epoch 489/500  Running Loss : 38.77600860595703 and testacc : 22.78914451599121\n",
      "Epoch 490/500  Running Loss : 38.77490234375 and testacc : 22.803403854370117\n",
      "Epoch 491/500  Running Loss : 38.773799896240234 and testacc : 22.817617416381836\n",
      "Epoch 492/500  Running Loss : 38.772705078125 and testacc : 22.831798553466797\n",
      "Epoch 493/500  Running Loss : 38.77162551879883 and testacc : 22.845949172973633\n",
      "Epoch 494/500  Running Loss : 38.770545959472656 and testacc : 22.86005210876465\n",
      "Epoch 495/500  Running Loss : 38.76947784423828 and testacc : 22.874095916748047\n",
      "Epoch 496/500  Running Loss : 38.76841354370117 and testacc : 22.88814926147461\n",
      "Epoch 497/500  Running Loss : 38.767356872558594 and testacc : 22.902141571044922\n",
      "Epoch 498/500  Running Loss : 38.76630783081055 and testacc : 22.91607666015625\n",
      "Epoch 499/500  Running Loss : 38.76526641845703 and testacc : 22.930004119873047\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  running_loss = 0.0\n",
    "  for index in range(0,len(train_batches)):\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(train_batches[index][0]).reshape([-1])\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs,train_batches[index][1])\n",
    "    running_loss += loss\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "  test_accuracy = outputs = criterion(model(test_x).reshape([-1]),test_y)\n",
    "  print(f\"Epoch {epoch}/{epochs}  Running Loss : {running_loss.item()/batch_size} and testacc : {test_accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ad352",
   "metadata": {},
   "source": [
    "<h1>Plaintext Inference</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0869e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "plaintext_predictions = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc3fbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  63.399288177490234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrishikesh/opt/anaconda3/envs/sympc/lib/python3.9/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([101])) that is different to the input size (torch.Size([101, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE Loss: \",criterion(plaintext_predictions,test_y).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b8c96",
   "metadata": {},
   "source": [
    "<h1>Encrypted Inference</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "314395b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from sympc.module.nn import mse_loss\n",
    "import sympc\n",
    "from sympc.session import Session\n",
    "from sympc.session import SessionManager\n",
    "from sympc.tensor import MPCTensor\n",
    "from sympc.optim import SGD\n",
    "from sympc.config import Config\n",
    "from sympc.protocol import Falcon,FSS\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e9096e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clients(n_parties):\n",
    "  # Define the virtual machines that would be use in the computation\n",
    "  parties=[]\n",
    "\n",
    "  for index in range(n_parties): \n",
    "      parties.append(sy.VirtualMachine(name = \"worker\"+str(index)).get_root_client())\n",
    "\n",
    "  return parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a394a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(n_clients,protocol):\n",
    "\n",
    "  parties=get_clients(n_clients)\n",
    "\n",
    "  # Setup the session for the computation\n",
    "  session = Session(parties = parties,protocol = protocol)\n",
    "  SessionManager.setup_mpc(session)\n",
    "\n",
    "  mpc_model = model.share(session)\n",
    "\n",
    "  test_data=MPCTensor(secret=test_x, session = session)\n",
    "\n",
    "  start_time = time.time()\n",
    "  enc_results = mpc_model(test_data)\n",
    "  end_time = time.time()\n",
    "\n",
    "  print(f\"Time for inference: {end_time-start_time}s\")\n",
    "\n",
    "  predictions = enc_results.reconstruct()\n",
    "    \n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d62c0ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.033750057220458984s\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,Falcon(\"semi-honest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0eb8b3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  tensor(63.3991)\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE Loss: \",criterion(predictions,test_y).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5cdd5",
   "metadata": {},
   "source": [
    "We can see that the prediction values and mean squared error values are almost the same as final model. Small differences are due to precision loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6be0ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0\n",
      "Encrypted Prediction Output 1.8402252197265625\n",
      "Plaintext Prediction Output 1.8402048349380493\n",
      "Expected Prediction: 5.0\n",
      "\n",
      "\n",
      "Index 1\n",
      "Encrypted Prediction Output 5.19158935546875\n",
      "Plaintext Prediction Output 5.191580772399902\n",
      "Expected Prediction: 11.8984375\n",
      "\n",
      "\n",
      "Index 2\n",
      "Encrypted Prediction Output 19.786148071289062\n",
      "Plaintext Prediction Output 19.786155700683594\n",
      "Expected Prediction: 27.90625\n",
      "\n",
      "\n",
      "Index 3\n",
      "Encrypted Prediction Output 13.1734619140625\n",
      "Plaintext Prediction Output 13.173490524291992\n",
      "Expected Prediction: 17.203125\n",
      "\n",
      "\n",
      "Index 4\n",
      "Encrypted Prediction Output 20.987045288085938\n",
      "Plaintext Prediction Output 20.987075805664062\n",
      "Expected Prediction: 27.5\n",
      "\n",
      "\n",
      "Index 5\n",
      "Encrypted Prediction Output 14.210540771484375\n",
      "Plaintext Prediction Output 14.210532188415527\n",
      "Expected Prediction: 15.0\n",
      "\n",
      "\n",
      "Index 6\n",
      "Encrypted Prediction Output 19.272994995117188\n",
      "Plaintext Prediction Output 19.273014068603516\n",
      "Expected Prediction: 17.203125\n",
      "\n",
      "\n",
      "Index 7\n",
      "Encrypted Prediction Output 1.7180023193359375\n",
      "Plaintext Prediction Output 1.717998743057251\n",
      "Expected Prediction: 17.90625\n",
      "\n",
      "\n",
      "Index 8\n",
      "Encrypted Prediction Output 10.214675903320312\n",
      "Plaintext Prediction Output 10.214686393737793\n",
      "Expected Prediction: 16.296875\n",
      "\n",
      "\n",
      "Index 9\n",
      "Encrypted Prediction Output -7.1717681884765625\n",
      "Plaintext Prediction Output -7.171802043914795\n",
      "Expected Prediction: 7.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(0,10):\n",
    "    print(f\"Index {index}\")\n",
    "    print(f\"Encrypted Prediction Output {predictions[index].item()}\")\n",
    "    print(f\"Plaintext Prediction Output {plaintext_predictions[index].item()}\")\n",
    "    print(f\"Expected Prediction: {test_y[index]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc2029",
   "metadata": {},
   "source": [
    "Falcon can also work with malicious security guarantee but at a large inference time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c930a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.5756688117980957s\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,Falcon(\"malicious\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "598ab19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-13T17:54:24.721347+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: e43609e4f3b24af1bab46d6a3463c6cf>.\n",
      "[2021-07-13T17:54:24.724769+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 433768ff3ded40c586a6b517735d8271>.\n",
      "[2021-07-13T17:54:24.726637+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 47ca2a8b0f55424b8c7dee7dc0a0ccb0>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.5407030582427979s\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,FSS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8ceef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-13T17:54:26.588460+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 384b45903a294c2e85c2b480de81ce4d>.\n",
      "[2021-07-13T17:54:26.595719+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 35ffa19d3ebf42ce9a3c5a0a619d59a9>.\n",
      "[2021-07-13T17:54:26.597316+0530][CRITICAL][logger]][31441] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 728a7f55e01c4ff98525a9b84f6a1cff>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.9237020015716553s\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(5,FSS())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04560264",
   "metadata": {},
   "source": [
    "\n",
    "<center> <h3> Comparison </h3> </center>\n",
    "\n",
    "| Protocol | Security Type| Parties | Inference Time (s) |\n",
    "| --- | --- | --- | --- |\n",
    "| Falcon | Semi-honest | 3 | 0.03391 |\n",
    "| Falcon | Malicious | 3 | 0.61146 |\n",
    "| FSS| Semi-honest | 3 | 0.56047 |\n",
    "| FSS | Semi-honest | 5 | 0.91995|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9830545",
   "metadata": {},
   "source": [
    "Falcon works for 3 parties and semi honest security setting provides significant security guarantee. For, N number of parties you can Functional Secret Sharing protocol (FSS). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42f165",
   "metadata": {},
   "source": [
    "<h3>What's next?</h3>\n",
    "\n",
    "SyMPC is still under development! We will add here more features as soon they are stable enough, stay tuned! ðŸ•º\n",
    "\n",
    "If you enjoyed this tutorial, show your support by Starring SyMPC! ðŸ™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049acf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
