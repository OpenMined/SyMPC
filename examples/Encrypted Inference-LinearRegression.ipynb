{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0537d283",
   "metadata": {},
   "source": [
    "<h1> Encrypted Inference-Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a6488",
   "metadata": {},
   "source": [
    "In this tutorial, we train a Linear regression model in plaintext on Boston Housing Dataset. Then we use the model for performing encrypted and plaintext inference on test data. This tutorial uses protocol Falcon for 3 parties and SPDZ for 3 and 5 parties. It depicts how you can perform encrypted inference on test data with nearly the same accuracy as plaintext. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbe39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a9d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2693cdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fabf4c432b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set a manual seed to maintain consistency\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64573ad",
   "metadata": {},
   "source": [
    "<h2>Data Loading and Processing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280adf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-13 19:26:30--  https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49082 (48K) [application/x-httpd-php]\n",
      "Saving to: ‘housing.data.1’\n",
      "\n",
      "housing.data.1      100%[===================>]  47.93K  70.2KB/s    in 0.7s    \n",
      "\n",
      "2021-07-13 19:26:32 (70.2 KB/s) - ‘housing.data.1’ saved [49082/49082]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a993d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "dataset=pd.read_csv(\"housing.data\",delim_whitespace=True,\n",
    "                    names=[\"crim\",\"zn\",\"indus\",\n",
    "                           \"chas\",\"nox\",\"rm\",\n",
    "                           \"age\",\"dis\",\"rad\",\n",
    "                           \"tax\",\"ptratio\",\"black\",\n",
    "                           \"lstat\",\"medv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0c0050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad    tax  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   ptratio   black  lstat  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualize and look at columns and rows of dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f893957",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = dataset.drop(\"medv\",axis=1)\n",
    "y_data = dataset[\"medv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b85b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e842f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: medv, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49c397f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419367</td>\n",
       "      <td>0.284548</td>\n",
       "      <td>-1.286636</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.144075</td>\n",
       "      <td>0.413263</td>\n",
       "      <td>-0.119895</td>\n",
       "      <td>0.140075</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.665949</td>\n",
       "      <td>-1.457558</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.074499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.416927</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>0.194082</td>\n",
       "      <td>0.366803</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.491953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.416929</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>1.281446</td>\n",
       "      <td>-0.265549</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.396035</td>\n",
       "      <td>-1.207532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416338</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.015298</td>\n",
       "      <td>-0.809088</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.415751</td>\n",
       "      <td>-1.360171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412074</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.227362</td>\n",
       "      <td>-0.510674</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.025487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>-0.412820</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.438881</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>-0.625178</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.386834</td>\n",
       "      <td>-0.417734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>-0.414839</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>-0.234316</td>\n",
       "      <td>0.288648</td>\n",
       "      <td>-0.715931</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.500355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>-0.413038</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.983986</td>\n",
       "      <td>0.796661</td>\n",
       "      <td>-0.772919</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.982076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>-0.407361</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.724955</td>\n",
       "      <td>0.736268</td>\n",
       "      <td>-0.667776</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.402826</td>\n",
       "      <td>-0.864446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>-0.414590</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>-0.362408</td>\n",
       "      <td>0.434302</td>\n",
       "      <td>-0.612640</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.668397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         crim        zn     indus      chas       nox        rm       age  \\\n",
       "0   -0.419367  0.284548 -1.286636 -0.272329 -0.144075  0.413263 -0.119895   \n",
       "1   -0.416927 -0.487240 -0.592794 -0.272329 -0.739530  0.194082  0.366803   \n",
       "2   -0.416929 -0.487240 -0.592794 -0.272329 -0.739530  1.281446 -0.265549   \n",
       "3   -0.416338 -0.487240 -1.305586 -0.272329 -0.834458  1.015298 -0.809088   \n",
       "4   -0.412074 -0.487240 -1.305586 -0.272329 -0.834458  1.227362 -0.510674   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "501 -0.412820 -0.487240  0.115624 -0.272329  0.157968  0.438881  0.018654   \n",
       "502 -0.414839 -0.487240  0.115624 -0.272329  0.157968 -0.234316  0.288648   \n",
       "503 -0.413038 -0.487240  0.115624 -0.272329  0.157968  0.983986  0.796661   \n",
       "504 -0.407361 -0.487240  0.115624 -0.272329  0.157968  0.724955  0.736268   \n",
       "505 -0.414590 -0.487240  0.115624 -0.272329  0.157968 -0.362408  0.434302   \n",
       "\n",
       "          dis       rad       tax   ptratio     black     lstat  \n",
       "0    0.140075 -0.981871 -0.665949 -1.457558  0.440616 -1.074499  \n",
       "1    0.556609 -0.867024 -0.986353 -0.302794  0.440616 -0.491953  \n",
       "2    0.556609 -0.867024 -0.986353 -0.302794  0.396035 -1.207532  \n",
       "3    1.076671 -0.752178 -1.105022  0.112920  0.415751 -1.360171  \n",
       "4    1.076671 -0.752178 -1.105022  0.112920  0.440616 -1.025487  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "501 -0.625178 -0.981871 -0.802418  1.175303  0.386834 -0.417734  \n",
       "502 -0.715931 -0.981871 -0.802418  1.175303  0.440616 -0.500355  \n",
       "503 -0.772919 -0.981871 -0.802418  1.175303  0.440616 -0.982076  \n",
       "504 -0.667776 -0.981871 -0.802418  1.175303  0.402826 -0.864446  \n",
       "505 -0.612640 -0.981871 -0.802418  1.175303  0.440616 -0.668397  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e938bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(X_data.values.astype(np.float16).astype(np.float32)) \n",
    "targets = torch.tensor(y_data.values.astype(np.float16).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f86c02a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4194,  0.2847, -1.2871,  ..., -1.4580,  0.4407, -1.0742],\n",
       "        [-0.4170, -0.4873, -0.5928,  ..., -0.3027,  0.4407, -0.4919],\n",
       "        [-0.4170, -0.4873, -0.5928,  ..., -0.3027,  0.3960, -1.2080],\n",
       "        ...,\n",
       "        [-0.4131, -0.4873,  0.1156,  ...,  1.1758,  0.4407, -0.9819],\n",
       "        [-0.4075, -0.4873,  0.1156,  ...,  1.1758,  0.4028, -0.8643],\n",
       "        [-0.4146, -0.4873,  0.1156,  ...,  1.1758,  0.4407, -0.6685]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21f40e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments of projects\n",
    "batch_size = 16\n",
    "epochs = 500\n",
    "train_test_split = 0.8\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bfae2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features[:int(len(features)*train_test_split)]\n",
    "train_y = targets[:int(len(features)*train_test_split)]\n",
    "\n",
    "test_x = features[int(len(features)*train_test_split)+1:]\n",
    "test_y = targets[int(len(features)*train_test_split)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13c89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X,y):\n",
    "    batches = []\n",
    "    for index in range(0,len(train_x)+1,batch_size):\n",
    "        batches.append((X[index:index+batch_size],y[index:index+batch_size]))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "781448fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches=get_batches(train_x,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c8e37",
   "metadata": {},
   "source": [
    "<h1>Plaintext Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf48b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import syft\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a2b9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Linear regression model\n",
    "class LinearSyNet(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(LinearSyNet, self).__init__(torch_ref=torch_ref)\n",
    "        self.fc1 = self.torch_ref.nn.Linear(13,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c476e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model, loss function and optimizer\n",
    "model = LinearSyNet(torch)\n",
    "criterion = torch.nn.MSELoss(reduction='mean') \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "428cbbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500  Running Loss : 978.0725708007812 and test loss: 367.93927001953125\n",
      "Epoch 1/500  Running Loss : 831.0911865234375 and test loss: 411.168212890625\n",
      "Epoch 2/500  Running Loss : 720.55322265625 and test loss: 440.28497314453125\n",
      "Epoch 3/500  Running Loss : 633.6892700195312 and test loss: 457.1828308105469\n",
      "Epoch 4/500  Running Loss : 562.962646484375 and test loss: 464.29931640625\n",
      "Epoch 5/500  Running Loss : 503.80291748046875 and test loss: 463.9801330566406\n",
      "Epoch 6/500  Running Loss : 453.3403015136719 and test loss: 458.2357177734375\n",
      "Epoch 7/500  Running Loss : 409.6958312988281 and test loss: 448.6855773925781\n",
      "Epoch 8/500  Running Loss : 371.5797424316406 and test loss: 436.5837707519531\n",
      "Epoch 9/500  Running Loss : 338.0628967285156 and test loss: 422.87548828125\n",
      "Epoch 10/500  Running Loss : 308.4455261230469 and test loss: 408.2576904296875\n",
      "Epoch 11/500  Running Loss : 282.1790466308594 and test loss: 393.2343444824219\n",
      "Epoch 12/500  Running Loss : 258.81988525390625 and test loss: 378.1633605957031\n",
      "Epoch 13/500  Running Loss : 238.00051879882812 and test loss: 363.2930603027344\n",
      "Epoch 14/500  Running Loss : 219.4108123779297 and test loss: 348.7910461425781\n",
      "Epoch 15/500  Running Loss : 202.78579711914062 and test loss: 334.7660827636719\n",
      "Epoch 16/500  Running Loss : 187.89691162109375 and test loss: 321.284912109375\n",
      "Epoch 17/500  Running Loss : 174.5458221435547 and test loss: 308.38336181640625\n",
      "Epoch 18/500  Running Loss : 162.55923461914062 and test loss: 296.076416015625\n",
      "Epoch 19/500  Running Loss : 151.7855987548828 and test loss: 284.3641357421875\n",
      "Epoch 20/500  Running Loss : 142.091552734375 and test loss: 273.23583984375\n",
      "Epoch 21/500  Running Loss : 133.3596649169922 and test loss: 262.6748352050781\n",
      "Epoch 22/500  Running Loss : 125.48639678955078 and test loss: 252.6597900390625\n",
      "Epoch 23/500  Running Loss : 118.38018798828125 and test loss: 243.16697692871094\n",
      "Epoch 24/500  Running Loss : 111.96000671386719 and test loss: 234.17144775390625\n",
      "Epoch 25/500  Running Loss : 106.15399932861328 and test loss: 225.6476593017578\n",
      "Epoch 26/500  Running Loss : 100.89836883544922 and test loss: 217.5703582763672\n",
      "Epoch 27/500  Running Loss : 96.13651275634766 and test loss: 209.9147186279297\n",
      "Epoch 28/500  Running Loss : 91.81795501708984 and test loss: 202.65699768066406\n",
      "Epoch 29/500  Running Loss : 87.89775085449219 and test loss: 195.77415466308594\n",
      "Epoch 30/500  Running Loss : 84.33589172363281 and test loss: 189.2443389892578\n",
      "Epoch 31/500  Running Loss : 81.09656524658203 and test loss: 183.04678344726562\n",
      "Epoch 32/500  Running Loss : 78.14794158935547 and test loss: 177.16195678710938\n",
      "Epoch 33/500  Running Loss : 75.46138763427734 and test loss: 171.57127380371094\n",
      "Epoch 34/500  Running Loss : 73.01140594482422 and test loss: 166.25747680664062\n",
      "Epoch 35/500  Running Loss : 70.77510070800781 and test loss: 161.2041473388672\n",
      "Epoch 36/500  Running Loss : 68.7318344116211 and test loss: 156.39573669433594\n",
      "Epoch 37/500  Running Loss : 66.86328125 and test loss: 151.8181915283203\n",
      "Epoch 38/500  Running Loss : 65.15292358398438 and test loss: 147.45770263671875\n",
      "Epoch 39/500  Running Loss : 63.585838317871094 and test loss: 143.3018035888672\n",
      "Epoch 40/500  Running Loss : 62.14863967895508 and test loss: 139.33837890625\n",
      "Epoch 41/500  Running Loss : 60.829307556152344 and test loss: 135.55654907226562\n",
      "Epoch 42/500  Running Loss : 59.61700439453125 and test loss: 131.94569396972656\n",
      "Epoch 43/500  Running Loss : 58.50190734863281 and test loss: 128.4962615966797\n",
      "Epoch 44/500  Running Loss : 57.47517776489258 and test loss: 125.1988525390625\n",
      "Epoch 45/500  Running Loss : 56.52889633178711 and test loss: 122.04502868652344\n",
      "Epoch 46/500  Running Loss : 55.655853271484375 and test loss: 119.02680969238281\n",
      "Epoch 47/500  Running Loss : 54.84951400756836 and test loss: 116.136474609375\n",
      "Epoch 48/500  Running Loss : 54.103973388671875 and test loss: 113.36707305908203\n",
      "Epoch 49/500  Running Loss : 53.41393280029297 and test loss: 110.7120361328125\n",
      "Epoch 50/500  Running Loss : 52.774539947509766 and test loss: 108.1650619506836\n",
      "Epoch 51/500  Running Loss : 52.18141555786133 and test loss: 105.72029113769531\n",
      "Epoch 52/500  Running Loss : 51.63058853149414 and test loss: 103.37239074707031\n",
      "Epoch 53/500  Running Loss : 51.118465423583984 and test loss: 101.11607360839844\n",
      "Epoch 54/500  Running Loss : 50.64175796508789 and test loss: 98.94660949707031\n",
      "Epoch 55/500  Running Loss : 50.197505950927734 and test loss: 96.85958099365234\n",
      "Epoch 56/500  Running Loss : 49.783016204833984 and test loss: 94.8506088256836\n",
      "Epoch 57/500  Running Loss : 49.39579772949219 and test loss: 92.91569519042969\n",
      "Epoch 58/500  Running Loss : 49.03364562988281 and test loss: 91.05118560791016\n",
      "Epoch 59/500  Running Loss : 48.694522857666016 and test loss: 89.25347137451172\n",
      "Epoch 60/500  Running Loss : 48.37654113769531 and test loss: 87.51924133300781\n",
      "Epoch 61/500  Running Loss : 48.0779914855957 and test loss: 85.84532928466797\n",
      "Epoch 62/500  Running Loss : 47.79737854003906 and test loss: 84.22891235351562\n",
      "Epoch 63/500  Running Loss : 47.53329086303711 and test loss: 82.66725158691406\n",
      "Epoch 64/500  Running Loss : 47.284420013427734 and test loss: 81.15760040283203\n",
      "Epoch 65/500  Running Loss : 47.049598693847656 and test loss: 79.69763946533203\n",
      "Epoch 66/500  Running Loss : 46.827754974365234 and test loss: 78.2849349975586\n",
      "Epoch 67/500  Running Loss : 46.617897033691406 and test loss: 76.91742706298828\n",
      "Epoch 68/500  Running Loss : 46.419151306152344 and test loss: 75.59300231933594\n",
      "Epoch 69/500  Running Loss : 46.230655670166016 and test loss: 74.30975341796875\n",
      "Epoch 70/500  Running Loss : 46.051700592041016 and test loss: 73.06578063964844\n",
      "Epoch 71/500  Running Loss : 45.88155746459961 and test loss: 71.8593978881836\n",
      "Epoch 72/500  Running Loss : 45.719608306884766 and test loss: 70.68897247314453\n",
      "Epoch 73/500  Running Loss : 45.56525802612305 and test loss: 69.55294036865234\n",
      "Epoch 74/500  Running Loss : 45.41799545288086 and test loss: 68.44985961914062\n",
      "Epoch 75/500  Running Loss : 45.27731704711914 and test loss: 67.3782958984375\n",
      "Epoch 76/500  Running Loss : 45.14276885986328 and test loss: 66.3370361328125\n",
      "Epoch 77/500  Running Loss : 45.01395797729492 and test loss: 65.32485961914062\n",
      "Epoch 78/500  Running Loss : 44.890472412109375 and test loss: 64.34046936035156\n",
      "Epoch 79/500  Running Loss : 44.77199172973633 and test loss: 63.38288879394531\n",
      "Epoch 80/500  Running Loss : 44.658180236816406 and test loss: 62.45102310180664\n",
      "Epoch 81/500  Running Loss : 44.54874038696289 and test loss: 61.54381561279297\n",
      "Epoch 82/500  Running Loss : 44.44339370727539 and test loss: 60.660343170166016\n",
      "Epoch 83/500  Running Loss : 44.34188461303711 and test loss: 59.79975128173828\n",
      "Epoch 84/500  Running Loss : 44.243988037109375 and test loss: 58.961185455322266\n",
      "Epoch 85/500  Running Loss : 44.14947509765625 and test loss: 58.14378356933594\n",
      "Epoch 86/500  Running Loss : 44.05816650390625 and test loss: 57.34678649902344\n",
      "Epoch 87/500  Running Loss : 43.96985626220703 and test loss: 56.56957244873047\n",
      "Epoch 88/500  Running Loss : 43.88438415527344 and test loss: 55.81130599975586\n",
      "Epoch 89/500  Running Loss : 43.80158996582031 and test loss: 55.071327209472656\n",
      "Epoch 90/500  Running Loss : 43.7213020324707 and test loss: 54.349029541015625\n",
      "Epoch 91/500  Running Loss : 43.6434326171875 and test loss: 53.64389419555664\n",
      "Epoch 92/500  Running Loss : 43.567813873291016 and test loss: 52.95525360107422\n",
      "Epoch 93/500  Running Loss : 43.494346618652344 and test loss: 52.28257369995117\n",
      "Epoch 94/500  Running Loss : 43.42291259765625 and test loss: 51.625404357910156\n",
      "Epoch 95/500  Running Loss : 43.35342025756836 and test loss: 50.98322296142578\n",
      "Epoch 96/500  Running Loss : 43.285762786865234 and test loss: 50.3554801940918\n",
      "Epoch 97/500  Running Loss : 43.21986770629883 and test loss: 49.74178695678711\n",
      "Epoch 98/500  Running Loss : 43.155635833740234 and test loss: 49.14167022705078\n",
      "Epoch 99/500  Running Loss : 43.09299087524414 and test loss: 48.55470657348633\n",
      "Epoch 100/500  Running Loss : 43.0318603515625 and test loss: 47.98057174682617\n",
      "Epoch 101/500  Running Loss : 42.97218704223633 and test loss: 47.418861389160156\n",
      "Epoch 102/500  Running Loss : 42.91390609741211 and test loss: 46.869140625\n",
      "Epoch 103/500  Running Loss : 42.856937408447266 and test loss: 46.33111572265625\n",
      "Epoch 104/500  Running Loss : 42.80125427246094 and test loss: 45.804500579833984\n",
      "Epoch 105/500  Running Loss : 42.746795654296875 and test loss: 45.28887939453125\n",
      "Epoch 106/500  Running Loss : 42.69351577758789 and test loss: 44.784088134765625\n",
      "Epoch 107/500  Running Loss : 42.641353607177734 and test loss: 44.28965759277344\n",
      "Epoch 108/500  Running Loss : 42.59027862548828 and test loss: 43.80537414550781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500  Running Loss : 42.54023361206055 and test loss: 43.331031799316406\n",
      "Epoch 110/500  Running Loss : 42.491207122802734 and test loss: 42.86627960205078\n",
      "Epoch 111/500  Running Loss : 42.44314956665039 and test loss: 42.410919189453125\n",
      "Epoch 112/500  Running Loss : 42.39602279663086 and test loss: 41.96467590332031\n",
      "Epoch 113/500  Running Loss : 42.34979248046875 and test loss: 41.527347564697266\n",
      "Epoch 114/500  Running Loss : 42.304443359375 and test loss: 41.09870147705078\n",
      "Epoch 115/500  Running Loss : 42.259925842285156 and test loss: 40.67849349975586\n",
      "Epoch 116/500  Running Loss : 42.21622848510742 and test loss: 40.26654815673828\n",
      "Epoch 117/500  Running Loss : 42.17331314086914 and test loss: 39.86261749267578\n",
      "Epoch 118/500  Running Loss : 42.131160736083984 and test loss: 39.466590881347656\n",
      "Epoch 119/500  Running Loss : 42.089759826660156 and test loss: 39.0782356262207\n",
      "Epoch 120/500  Running Loss : 42.04906463623047 and test loss: 38.69734573364258\n",
      "Epoch 121/500  Running Loss : 42.00907516479492 and test loss: 38.323787689208984\n",
      "Epoch 122/500  Running Loss : 41.96976089477539 and test loss: 37.95735549926758\n",
      "Epoch 123/500  Running Loss : 41.93108367919922 and test loss: 37.597877502441406\n",
      "Epoch 124/500  Running Loss : 41.893070220947266 and test loss: 37.24527359008789\n",
      "Epoch 125/500  Running Loss : 41.85566711425781 and test loss: 36.89934539794922\n",
      "Epoch 126/500  Running Loss : 41.818870544433594 and test loss: 36.5599479675293\n",
      "Epoch 127/500  Running Loss : 41.78266525268555 and test loss: 36.22693634033203\n",
      "Epoch 128/500  Running Loss : 41.74702453613281 and test loss: 35.90016174316406\n",
      "Epoch 129/500  Running Loss : 41.71194839477539 and test loss: 35.579463958740234\n",
      "Epoch 130/500  Running Loss : 41.67741775512695 and test loss: 35.264793395996094\n",
      "Epoch 131/500  Running Loss : 41.64341354370117 and test loss: 34.95596694946289\n",
      "Epoch 132/500  Running Loss : 41.609928131103516 and test loss: 34.652870178222656\n",
      "Epoch 133/500  Running Loss : 41.576942443847656 and test loss: 34.35540008544922\n",
      "Epoch 134/500  Running Loss : 41.544464111328125 and test loss: 34.063453674316406\n",
      "Epoch 135/500  Running Loss : 41.5124626159668 and test loss: 33.776878356933594\n",
      "Epoch 136/500  Running Loss : 41.48093032836914 and test loss: 33.49558639526367\n",
      "Epoch 137/500  Running Loss : 41.44984436035156 and test loss: 33.2194938659668\n",
      "Epoch 138/500  Running Loss : 41.419227600097656 and test loss: 32.948429107666016\n",
      "Epoch 139/500  Running Loss : 41.3890380859375 and test loss: 32.68235778808594\n",
      "Epoch 140/500  Running Loss : 41.35927963256836 and test loss: 32.4211540222168\n",
      "Epoch 141/500  Running Loss : 41.32994079589844 and test loss: 32.164710998535156\n",
      "Epoch 142/500  Running Loss : 41.301021575927734 and test loss: 31.912988662719727\n",
      "Epoch 143/500  Running Loss : 41.27251052856445 and test loss: 31.66588592529297\n",
      "Epoch 144/500  Running Loss : 41.244384765625 and test loss: 31.42328453063965\n",
      "Epoch 145/500  Running Loss : 41.21664810180664 and test loss: 31.185121536254883\n",
      "Epoch 146/500  Running Loss : 41.18929672241211 and test loss: 30.95130157470703\n",
      "Epoch 147/500  Running Loss : 41.16231155395508 and test loss: 30.721752166748047\n",
      "Epoch 148/500  Running Loss : 41.13569641113281 and test loss: 30.496389389038086\n",
      "Epoch 149/500  Running Loss : 41.10943603515625 and test loss: 30.275148391723633\n",
      "Epoch 150/500  Running Loss : 41.083518981933594 and test loss: 30.05791473388672\n",
      "Epoch 151/500  Running Loss : 41.05794906616211 and test loss: 29.844669342041016\n",
      "Epoch 152/500  Running Loss : 41.03272247314453 and test loss: 29.635345458984375\n",
      "Epoch 153/500  Running Loss : 41.007835388183594 and test loss: 29.4298038482666\n",
      "Epoch 154/500  Running Loss : 40.98326110839844 and test loss: 29.228036880493164\n",
      "Epoch 155/500  Running Loss : 40.95900344848633 and test loss: 29.029956817626953\n",
      "Epoch 156/500  Running Loss : 40.9350700378418 and test loss: 28.83550262451172\n",
      "Epoch 157/500  Running Loss : 40.911434173583984 and test loss: 28.644594192504883\n",
      "Epoch 158/500  Running Loss : 40.88811492919922 and test loss: 28.45717430114746\n",
      "Epoch 159/500  Running Loss : 40.865074157714844 and test loss: 28.27318572998047\n",
      "Epoch 160/500  Running Loss : 40.842342376708984 and test loss: 28.09259033203125\n",
      "Epoch 161/500  Running Loss : 40.819881439208984 and test loss: 27.91530418395996\n",
      "Epoch 162/500  Running Loss : 40.7977180480957 and test loss: 27.741268157958984\n",
      "Epoch 163/500  Running Loss : 40.775821685791016 and test loss: 27.570449829101562\n",
      "Epoch 164/500  Running Loss : 40.75420379638672 and test loss: 27.40277862548828\n",
      "Epoch 165/500  Running Loss : 40.732845306396484 and test loss: 27.23822593688965\n",
      "Epoch 166/500  Running Loss : 40.71176528930664 and test loss: 27.076698303222656\n",
      "Epoch 167/500  Running Loss : 40.69093704223633 and test loss: 26.91815757751465\n",
      "Epoch 168/500  Running Loss : 40.670372009277344 and test loss: 26.762569427490234\n",
      "Epoch 169/500  Running Loss : 40.650047302246094 and test loss: 26.609878540039062\n",
      "Epoch 170/500  Running Loss : 40.629974365234375 and test loss: 26.460018157958984\n",
      "Epoch 171/500  Running Loss : 40.61014175415039 and test loss: 26.312973022460938\n",
      "Epoch 172/500  Running Loss : 40.59054946899414 and test loss: 26.16868019104004\n",
      "Epoch 173/500  Running Loss : 40.57119369506836 and test loss: 26.027069091796875\n",
      "Epoch 174/500  Running Loss : 40.552066802978516 and test loss: 25.888141632080078\n",
      "Epoch 175/500  Running Loss : 40.533172607421875 and test loss: 25.751798629760742\n",
      "Epoch 176/500  Running Loss : 40.514495849609375 and test loss: 25.61804962158203\n",
      "Epoch 177/500  Running Loss : 40.49604797363281 and test loss: 25.486841201782227\n",
      "Epoch 178/500  Running Loss : 40.47781753540039 and test loss: 25.358123779296875\n",
      "Epoch 179/500  Running Loss : 40.45980453491211 and test loss: 25.23185157775879\n",
      "Epoch 180/500  Running Loss : 40.44198989868164 and test loss: 25.107990264892578\n",
      "Epoch 181/500  Running Loss : 40.424400329589844 and test loss: 24.98651123046875\n",
      "Epoch 182/500  Running Loss : 40.407005310058594 and test loss: 24.867368698120117\n",
      "Epoch 183/500  Running Loss : 40.38981628417969 and test loss: 24.750503540039062\n",
      "Epoch 184/500  Running Loss : 40.372825622558594 and test loss: 24.635879516601562\n",
      "Epoch 185/500  Running Loss : 40.35603713989258 and test loss: 24.52349281311035\n",
      "Epoch 186/500  Running Loss : 40.33943557739258 and test loss: 24.413293838500977\n",
      "Epoch 187/500  Running Loss : 40.323020935058594 and test loss: 24.305246353149414\n",
      "Epoch 188/500  Running Loss : 40.30680847167969 and test loss: 24.199321746826172\n",
      "Epoch 189/500  Running Loss : 40.290767669677734 and test loss: 24.095460891723633\n",
      "Epoch 190/500  Running Loss : 40.27491760253906 and test loss: 23.993669509887695\n",
      "Epoch 191/500  Running Loss : 40.259246826171875 and test loss: 23.893898010253906\n",
      "Epoch 192/500  Running Loss : 40.24375534057617 and test loss: 23.796104431152344\n",
      "Epoch 193/500  Running Loss : 40.228431701660156 and test loss: 23.700260162353516\n",
      "Epoch 194/500  Running Loss : 40.21328353881836 and test loss: 23.606338500976562\n",
      "Epoch 195/500  Running Loss : 40.19831466674805 and test loss: 23.514305114746094\n",
      "Epoch 196/500  Running Loss : 40.18351364135742 and test loss: 23.424110412597656\n",
      "Epoch 197/500  Running Loss : 40.16886901855469 and test loss: 23.33576774597168\n",
      "Epoch 198/500  Running Loss : 40.154388427734375 and test loss: 23.24924087524414\n",
      "Epoch 199/500  Running Loss : 40.140071868896484 and test loss: 23.164461135864258\n",
      "Epoch 200/500  Running Loss : 40.125911712646484 and test loss: 23.08142852783203\n",
      "Epoch 201/500  Running Loss : 40.11191177368164 and test loss: 23.000110626220703\n",
      "Epoch 202/500  Running Loss : 40.09806442260742 and test loss: 22.920501708984375\n",
      "Epoch 203/500  Running Loss : 40.084373474121094 and test loss: 22.84253692626953\n",
      "Epoch 204/500  Running Loss : 40.07083511352539 and test loss: 22.766212463378906\n",
      "Epoch 205/500  Running Loss : 40.057437896728516 and test loss: 22.69149398803711\n",
      "Epoch 206/500  Running Loss : 40.044185638427734 and test loss: 22.618364334106445\n",
      "Epoch 207/500  Running Loss : 40.031089782714844 and test loss: 22.546810150146484\n",
      "Epoch 208/500  Running Loss : 40.01811599731445 and test loss: 22.476770401000977\n",
      "Epoch 209/500  Running Loss : 40.00531005859375 and test loss: 22.40825843811035\n",
      "Epoch 210/500  Running Loss : 39.99263381958008 and test loss: 22.341228485107422\n",
      "Epoch 211/500  Running Loss : 39.9800910949707 and test loss: 22.275659561157227\n",
      "Epoch 212/500  Running Loss : 39.967674255371094 and test loss: 22.211524963378906\n",
      "Epoch 213/500  Running Loss : 39.95540237426758 and test loss: 22.14882469177246\n",
      "Epoch 214/500  Running Loss : 39.943267822265625 and test loss: 22.087520599365234\n",
      "Epoch 215/500  Running Loss : 39.93125915527344 and test loss: 22.0275821685791\n",
      "Epoch 216/500  Running Loss : 39.91938018798828 and test loss: 21.968982696533203\n",
      "Epoch 217/500  Running Loss : 39.90762710571289 and test loss: 21.911720275878906\n",
      "Epoch 218/500  Running Loss : 39.89599609375 and test loss: 21.855758666992188\n",
      "Epoch 219/500  Running Loss : 39.884490966796875 and test loss: 21.80109214782715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/500  Running Loss : 39.87311553955078 and test loss: 21.747699737548828\n",
      "Epoch 221/500  Running Loss : 39.861854553222656 and test loss: 21.6955623626709\n",
      "Epoch 222/500  Running Loss : 39.850711822509766 and test loss: 21.644638061523438\n",
      "Epoch 223/500  Running Loss : 39.83968734741211 and test loss: 21.594924926757812\n",
      "Epoch 224/500  Running Loss : 39.82878112792969 and test loss: 21.546396255493164\n",
      "Epoch 225/500  Running Loss : 39.817996978759766 and test loss: 21.49904441833496\n",
      "Epoch 226/500  Running Loss : 39.80730438232422 and test loss: 21.45285415649414\n",
      "Epoch 227/500  Running Loss : 39.79674530029297 and test loss: 21.407787322998047\n",
      "Epoch 228/500  Running Loss : 39.786285400390625 and test loss: 21.363840103149414\n",
      "Epoch 229/500  Running Loss : 39.77593231201172 and test loss: 21.32098960876465\n",
      "Epoch 230/500  Running Loss : 39.76569366455078 and test loss: 21.279216766357422\n",
      "Epoch 231/500  Running Loss : 39.75556182861328 and test loss: 21.238508224487305\n",
      "Epoch 232/500  Running Loss : 39.74553298950195 and test loss: 21.19884490966797\n",
      "Epoch 233/500  Running Loss : 39.73561477661133 and test loss: 21.16020965576172\n",
      "Epoch 234/500  Running Loss : 39.725799560546875 and test loss: 21.122589111328125\n",
      "Epoch 235/500  Running Loss : 39.7160758972168 and test loss: 21.08596420288086\n",
      "Epoch 236/500  Running Loss : 39.70645523071289 and test loss: 21.050317764282227\n",
      "Epoch 237/500  Running Loss : 39.696937561035156 and test loss: 21.015642166137695\n",
      "Epoch 238/500  Running Loss : 39.6875114440918 and test loss: 20.981918334960938\n",
      "Epoch 239/500  Running Loss : 39.67819595336914 and test loss: 20.949127197265625\n",
      "Epoch 240/500  Running Loss : 39.66896057128906 and test loss: 20.917255401611328\n",
      "Epoch 241/500  Running Loss : 39.659828186035156 and test loss: 20.88629150390625\n",
      "Epoch 242/500  Running Loss : 39.65079116821289 and test loss: 20.85620880126953\n",
      "Epoch 243/500  Running Loss : 39.6418342590332 and test loss: 20.826997756958008\n",
      "Epoch 244/500  Running Loss : 39.632972717285156 and test loss: 20.798662185668945\n",
      "Epoch 245/500  Running Loss : 39.62420654296875 and test loss: 20.771162033081055\n",
      "Epoch 246/500  Running Loss : 39.61553192138672 and test loss: 20.744508743286133\n",
      "Epoch 247/500  Running Loss : 39.60693359375 and test loss: 20.71866798400879\n",
      "Epoch 248/500  Running Loss : 39.59843063354492 and test loss: 20.69363021850586\n",
      "Epoch 249/500  Running Loss : 39.59001159667969 and test loss: 20.669389724731445\n",
      "Epoch 250/500  Running Loss : 39.58168411254883 and test loss: 20.645917892456055\n",
      "Epoch 251/500  Running Loss : 39.573429107666016 and test loss: 20.62322998046875\n",
      "Epoch 252/500  Running Loss : 39.56526565551758 and test loss: 20.601301193237305\n",
      "Epoch 253/500  Running Loss : 39.55717086791992 and test loss: 20.580106735229492\n",
      "Epoch 254/500  Running Loss : 39.54916763305664 and test loss: 20.559650421142578\n",
      "Epoch 255/500  Running Loss : 39.54125213623047 and test loss: 20.53989601135254\n",
      "Epoch 256/500  Running Loss : 39.53340148925781 and test loss: 20.52086639404297\n",
      "Epoch 257/500  Running Loss : 39.525630950927734 and test loss: 20.502532958984375\n",
      "Epoch 258/500  Running Loss : 39.5179443359375 and test loss: 20.484880447387695\n",
      "Epoch 259/500  Running Loss : 39.51033020019531 and test loss: 20.467897415161133\n",
      "Epoch 260/500  Running Loss : 39.50278854370117 and test loss: 20.45157814025879\n",
      "Epoch 261/500  Running Loss : 39.495323181152344 and test loss: 20.4359073638916\n",
      "Epoch 262/500  Running Loss : 39.48793411254883 and test loss: 20.420883178710938\n",
      "Epoch 263/500  Running Loss : 39.48061752319336 and test loss: 20.40648651123047\n",
      "Epoch 264/500  Running Loss : 39.473365783691406 and test loss: 20.3927059173584\n",
      "Epoch 265/500  Running Loss : 39.46619415283203 and test loss: 20.379535675048828\n",
      "Epoch 266/500  Running Loss : 39.45909118652344 and test loss: 20.366954803466797\n",
      "Epoch 267/500  Running Loss : 39.45205307006836 and test loss: 20.3549747467041\n",
      "Epoch 268/500  Running Loss : 39.445091247558594 and test loss: 20.343563079833984\n",
      "Epoch 269/500  Running Loss : 39.43818664550781 and test loss: 20.332725524902344\n",
      "Epoch 270/500  Running Loss : 39.43136215209961 and test loss: 20.322439193725586\n",
      "Epoch 271/500  Running Loss : 39.424591064453125 and test loss: 20.31270980834961\n",
      "Epoch 272/500  Running Loss : 39.41788864135742 and test loss: 20.303518295288086\n",
      "Epoch 273/500  Running Loss : 39.41126251220703 and test loss: 20.294851303100586\n",
      "Epoch 274/500  Running Loss : 39.404685974121094 and test loss: 20.286712646484375\n",
      "Epoch 275/500  Running Loss : 39.398189544677734 and test loss: 20.279090881347656\n",
      "Epoch 276/500  Running Loss : 39.391727447509766 and test loss: 20.271970748901367\n",
      "Epoch 277/500  Running Loss : 39.38535690307617 and test loss: 20.265352249145508\n",
      "Epoch 278/500  Running Loss : 39.37903594970703 and test loss: 20.259214401245117\n",
      "Epoch 279/500  Running Loss : 39.372772216796875 and test loss: 20.253557205200195\n",
      "Epoch 280/500  Running Loss : 39.366573333740234 and test loss: 20.248369216918945\n",
      "Epoch 281/500  Running Loss : 39.360435485839844 and test loss: 20.243650436401367\n",
      "Epoch 282/500  Running Loss : 39.354347229003906 and test loss: 20.239383697509766\n",
      "Epoch 283/500  Running Loss : 39.348331451416016 and test loss: 20.23556137084961\n",
      "Epoch 284/500  Running Loss : 39.34236145019531 and test loss: 20.2321720123291\n",
      "Epoch 285/500  Running Loss : 39.336456298828125 and test loss: 20.229219436645508\n",
      "Epoch 286/500  Running Loss : 39.330596923828125 and test loss: 20.22669219970703\n",
      "Epoch 287/500  Running Loss : 39.32480239868164 and test loss: 20.224578857421875\n",
      "Epoch 288/500  Running Loss : 39.31906509399414 and test loss: 20.222871780395508\n",
      "Epoch 289/500  Running Loss : 39.31336975097656 and test loss: 20.221569061279297\n",
      "Epoch 290/500  Running Loss : 39.307735443115234 and test loss: 20.220651626586914\n",
      "Epoch 291/500  Running Loss : 39.30215835571289 and test loss: 20.220134735107422\n",
      "Epoch 292/500  Running Loss : 39.29661560058594 and test loss: 20.219989776611328\n",
      "Epoch 293/500  Running Loss : 39.29114532470703 and test loss: 20.22022247314453\n",
      "Epoch 294/500  Running Loss : 39.285709381103516 and test loss: 20.220821380615234\n",
      "Epoch 295/500  Running Loss : 39.28034210205078 and test loss: 20.22177505493164\n",
      "Epoch 296/500  Running Loss : 39.2750129699707 and test loss: 20.22309112548828\n",
      "Epoch 297/500  Running Loss : 39.269737243652344 and test loss: 20.224748611450195\n",
      "Epoch 298/500  Running Loss : 39.2645149230957 and test loss: 20.22675132751465\n",
      "Epoch 299/500  Running Loss : 39.25933837890625 and test loss: 20.229074478149414\n",
      "Epoch 300/500  Running Loss : 39.25419998168945 and test loss: 20.23174476623535\n",
      "Epoch 301/500  Running Loss : 39.24912643432617 and test loss: 20.23473358154297\n",
      "Epoch 302/500  Running Loss : 39.244083404541016 and test loss: 20.23802947998047\n",
      "Epoch 303/500  Running Loss : 39.23908996582031 and test loss: 20.24164581298828\n",
      "Epoch 304/500  Running Loss : 39.23415756225586 and test loss: 20.24556541442871\n",
      "Epoch 305/500  Running Loss : 39.229251861572266 and test loss: 20.249784469604492\n",
      "Epoch 306/500  Running Loss : 39.22439956665039 and test loss: 20.25429344177246\n",
      "Epoch 307/500  Running Loss : 39.21958923339844 and test loss: 20.25909423828125\n",
      "Epoch 308/500  Running Loss : 39.21483612060547 and test loss: 20.264179229736328\n",
      "Epoch 309/500  Running Loss : 39.210105895996094 and test loss: 20.269533157348633\n",
      "Epoch 310/500  Running Loss : 39.20543670654297 and test loss: 20.27516746520996\n",
      "Epoch 311/500  Running Loss : 39.200801849365234 and test loss: 20.28106689453125\n",
      "Epoch 312/500  Running Loss : 39.19620132446289 and test loss: 20.28721809387207\n",
      "Epoch 313/500  Running Loss : 39.191654205322266 and test loss: 20.293636322021484\n",
      "Epoch 314/500  Running Loss : 39.18714141845703 and test loss: 20.300304412841797\n",
      "Epoch 315/500  Running Loss : 39.18266677856445 and test loss: 20.307220458984375\n",
      "Epoch 316/500  Running Loss : 39.17824935913086 and test loss: 20.314380645751953\n",
      "Epoch 317/500  Running Loss : 39.173858642578125 and test loss: 20.3217716217041\n",
      "Epoch 318/500  Running Loss : 39.16950607299805 and test loss: 20.32940101623535\n",
      "Epoch 319/500  Running Loss : 39.165199279785156 and test loss: 20.337257385253906\n",
      "Epoch 320/500  Running Loss : 39.160919189453125 and test loss: 20.34532928466797\n",
      "Epoch 321/500  Running Loss : 39.15670394897461 and test loss: 20.35363006591797\n",
      "Epoch 322/500  Running Loss : 39.15250015258789 and test loss: 20.36213493347168\n",
      "Epoch 323/500  Running Loss : 39.148345947265625 and test loss: 20.3708553314209\n",
      "Epoch 324/500  Running Loss : 39.144222259521484 and test loss: 20.379789352416992\n",
      "Epoch 325/500  Running Loss : 39.140140533447266 and test loss: 20.388916015625\n",
      "Epoch 326/500  Running Loss : 39.13609313964844 and test loss: 20.398235321044922\n",
      "Epoch 327/500  Running Loss : 39.132080078125 and test loss: 20.407751083374023\n",
      "Epoch 328/500  Running Loss : 39.128108978271484 and test loss: 20.417455673217773\n",
      "Epoch 329/500  Running Loss : 39.12416458129883 and test loss: 20.427345275878906\n",
      "Epoch 330/500  Running Loss : 39.120262145996094 and test loss: 20.43741798400879\n",
      "Epoch 331/500  Running Loss : 39.11638641357422 and test loss: 20.447662353515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332/500  Running Loss : 39.112548828125 and test loss: 20.458087921142578\n",
      "Epoch 333/500  Running Loss : 39.10874557495117 and test loss: 20.46868324279785\n",
      "Epoch 334/500  Running Loss : 39.104976654052734 and test loss: 20.47944450378418\n",
      "Epoch 335/500  Running Loss : 39.10124206542969 and test loss: 20.490375518798828\n",
      "Epoch 336/500  Running Loss : 39.09754180908203 and test loss: 20.501455307006836\n",
      "Epoch 337/500  Running Loss : 39.09386444091797 and test loss: 20.5126895904541\n",
      "Epoch 338/500  Running Loss : 39.090232849121094 and test loss: 20.524080276489258\n",
      "Epoch 339/500  Running Loss : 39.08661651611328 and test loss: 20.535627365112305\n",
      "Epoch 340/500  Running Loss : 39.08304977416992 and test loss: 20.547306060791016\n",
      "Epoch 341/500  Running Loss : 39.07950210571289 and test loss: 20.55913543701172\n",
      "Epoch 342/500  Running Loss : 39.07598114013672 and test loss: 20.57109832763672\n",
      "Epoch 343/500  Running Loss : 39.0724983215332 and test loss: 20.583200454711914\n",
      "Epoch 344/500  Running Loss : 39.06904220581055 and test loss: 20.595434188842773\n",
      "Epoch 345/500  Running Loss : 39.06562042236328 and test loss: 20.607799530029297\n",
      "Epoch 346/500  Running Loss : 39.06222915649414 and test loss: 20.620298385620117\n",
      "Epoch 347/500  Running Loss : 39.05885314941406 and test loss: 20.632902145385742\n",
      "Epoch 348/500  Running Loss : 39.05553436279297 and test loss: 20.64563751220703\n",
      "Epoch 349/500  Running Loss : 39.05221939086914 and test loss: 20.658483505249023\n",
      "Epoch 350/500  Running Loss : 39.0489387512207 and test loss: 20.67145347595215\n",
      "Epoch 351/500  Running Loss : 39.04568862915039 and test loss: 20.684532165527344\n",
      "Epoch 352/500  Running Loss : 39.0424690246582 and test loss: 20.69772720336914\n",
      "Epoch 353/500  Running Loss : 39.03926467895508 and test loss: 20.71101951599121\n",
      "Epoch 354/500  Running Loss : 39.03609848022461 and test loss: 20.724411010742188\n",
      "Epoch 355/500  Running Loss : 39.032955169677734 and test loss: 20.73789405822754\n",
      "Epoch 356/500  Running Loss : 39.02984619140625 and test loss: 20.751489639282227\n",
      "Epoch 357/500  Running Loss : 39.02676010131836 and test loss: 20.765193939208984\n",
      "Epoch 358/500  Running Loss : 39.023685455322266 and test loss: 20.778980255126953\n",
      "Epoch 359/500  Running Loss : 39.020652770996094 and test loss: 20.792858123779297\n",
      "Epoch 360/500  Running Loss : 39.01765060424805 and test loss: 20.80681037902832\n",
      "Epoch 361/500  Running Loss : 39.01465606689453 and test loss: 20.820865631103516\n",
      "Epoch 362/500  Running Loss : 39.011695861816406 and test loss: 20.83498191833496\n",
      "Epoch 363/500  Running Loss : 39.008766174316406 and test loss: 20.84920883178711\n",
      "Epoch 364/500  Running Loss : 39.005859375 and test loss: 20.863489151000977\n",
      "Epoch 365/500  Running Loss : 39.00296401977539 and test loss: 20.877857208251953\n",
      "Epoch 366/500  Running Loss : 39.000099182128906 and test loss: 20.89231300354004\n",
      "Epoch 367/500  Running Loss : 38.99726867675781 and test loss: 20.906829833984375\n",
      "Epoch 368/500  Running Loss : 38.99444580078125 and test loss: 20.921415328979492\n",
      "Epoch 369/500  Running Loss : 38.991661071777344 and test loss: 20.936071395874023\n",
      "Epoch 370/500  Running Loss : 38.9888916015625 and test loss: 20.950786590576172\n",
      "Epoch 371/500  Running Loss : 38.986148834228516 and test loss: 20.965574264526367\n",
      "Epoch 372/500  Running Loss : 38.983421325683594 and test loss: 20.98041534423828\n",
      "Epoch 373/500  Running Loss : 38.98072052001953 and test loss: 20.99534034729004\n",
      "Epoch 374/500  Running Loss : 38.97804260253906 and test loss: 21.01030731201172\n",
      "Epoch 375/500  Running Loss : 38.97539138793945 and test loss: 21.025327682495117\n",
      "Epoch 376/500  Running Loss : 38.97275161743164 and test loss: 21.0404109954834\n",
      "Epoch 377/500  Running Loss : 38.970130920410156 and test loss: 21.05553436279297\n",
      "Epoch 378/500  Running Loss : 38.9675407409668 and test loss: 21.070709228515625\n",
      "Epoch 379/500  Running Loss : 38.9649772644043 and test loss: 21.0859375\n",
      "Epoch 380/500  Running Loss : 38.96242904663086 and test loss: 21.10120964050293\n",
      "Epoch 381/500  Running Loss : 38.959896087646484 and test loss: 21.116533279418945\n",
      "Epoch 382/500  Running Loss : 38.9573860168457 and test loss: 21.131898880004883\n",
      "Epoch 383/500  Running Loss : 38.95490646362305 and test loss: 21.14729881286621\n",
      "Epoch 384/500  Running Loss : 38.95243453979492 and test loss: 21.162734985351562\n",
      "Epoch 385/500  Running Loss : 38.94998550415039 and test loss: 21.178211212158203\n",
      "Epoch 386/500  Running Loss : 38.94756317138672 and test loss: 21.193729400634766\n",
      "Epoch 387/500  Running Loss : 38.945152282714844 and test loss: 21.209272384643555\n",
      "Epoch 388/500  Running Loss : 38.94276809692383 and test loss: 21.2248477935791\n",
      "Epoch 389/500  Running Loss : 38.940399169921875 and test loss: 21.240463256835938\n",
      "Epoch 390/500  Running Loss : 38.938053131103516 and test loss: 21.256101608276367\n",
      "Epoch 391/500  Running Loss : 38.935726165771484 and test loss: 21.271774291992188\n",
      "Epoch 392/500  Running Loss : 38.93341064453125 and test loss: 21.28748321533203\n",
      "Epoch 393/500  Running Loss : 38.931114196777344 and test loss: 21.303213119506836\n",
      "Epoch 394/500  Running Loss : 38.92884063720703 and test loss: 21.318967819213867\n",
      "Epoch 395/500  Running Loss : 38.92658233642578 and test loss: 21.334760665893555\n",
      "Epoch 396/500  Running Loss : 38.924339294433594 and test loss: 21.350555419921875\n",
      "Epoch 397/500  Running Loss : 38.922122955322266 and test loss: 21.366374969482422\n",
      "Epoch 398/500  Running Loss : 38.919918060302734 and test loss: 21.382217407226562\n",
      "Epoch 399/500  Running Loss : 38.917728424072266 and test loss: 21.398086547851562\n",
      "Epoch 400/500  Running Loss : 38.91556167602539 and test loss: 21.413959503173828\n",
      "Epoch 401/500  Running Loss : 38.913414001464844 and test loss: 21.429841995239258\n",
      "Epoch 402/500  Running Loss : 38.911277770996094 and test loss: 21.44574737548828\n",
      "Epoch 403/500  Running Loss : 38.909156799316406 and test loss: 21.461660385131836\n",
      "Epoch 404/500  Running Loss : 38.90705871582031 and test loss: 21.477609634399414\n",
      "Epoch 405/500  Running Loss : 38.90497589111328 and test loss: 21.49355125427246\n",
      "Epoch 406/500  Running Loss : 38.90290069580078 and test loss: 21.509506225585938\n",
      "Epoch 407/500  Running Loss : 38.900856018066406 and test loss: 21.52546501159668\n",
      "Epoch 408/500  Running Loss : 38.89881896972656 and test loss: 21.54142951965332\n",
      "Epoch 409/500  Running Loss : 38.896793365478516 and test loss: 21.557397842407227\n",
      "Epoch 410/500  Running Loss : 38.89479064941406 and test loss: 21.573379516601562\n",
      "Epoch 411/500  Running Loss : 38.89280319213867 and test loss: 21.5893611907959\n",
      "Epoch 412/500  Running Loss : 38.890830993652344 and test loss: 21.60536766052246\n",
      "Epoch 413/500  Running Loss : 38.88886642456055 and test loss: 21.621353149414062\n",
      "Epoch 414/500  Running Loss : 38.88692092895508 and test loss: 21.637344360351562\n",
      "Epoch 415/500  Running Loss : 38.885005950927734 and test loss: 21.653339385986328\n",
      "Epoch 416/500  Running Loss : 38.88309097290039 and test loss: 21.6693172454834\n",
      "Epoch 417/500  Running Loss : 38.881195068359375 and test loss: 21.685321807861328\n",
      "Epoch 418/500  Running Loss : 38.87931442260742 and test loss: 21.70130729675293\n",
      "Epoch 419/500  Running Loss : 38.877437591552734 and test loss: 21.717300415039062\n",
      "Epoch 420/500  Running Loss : 38.875587463378906 and test loss: 21.7332706451416\n",
      "Epoch 421/500  Running Loss : 38.873748779296875 and test loss: 21.74924659729004\n",
      "Epoch 422/500  Running Loss : 38.871917724609375 and test loss: 21.765233993530273\n",
      "Epoch 423/500  Running Loss : 38.8701057434082 and test loss: 21.781192779541016\n",
      "Epoch 424/500  Running Loss : 38.868309020996094 and test loss: 21.797147750854492\n",
      "Epoch 425/500  Running Loss : 38.86652755737305 and test loss: 21.813098907470703\n",
      "Epoch 426/500  Running Loss : 38.86475372314453 and test loss: 21.829029083251953\n",
      "Epoch 427/500  Running Loss : 38.863006591796875 and test loss: 21.84494972229004\n",
      "Epoch 428/500  Running Loss : 38.86125183105469 and test loss: 21.860876083374023\n",
      "Epoch 429/500  Running Loss : 38.859527587890625 and test loss: 21.876792907714844\n",
      "Epoch 430/500  Running Loss : 38.85780334472656 and test loss: 21.892681121826172\n",
      "Epoch 431/500  Running Loss : 38.856101989746094 and test loss: 21.90856170654297\n",
      "Epoch 432/500  Running Loss : 38.85441207885742 and test loss: 21.924440383911133\n",
      "Epoch 433/500  Running Loss : 38.85272979736328 and test loss: 21.94029426574707\n",
      "Epoch 434/500  Running Loss : 38.85105895996094 and test loss: 21.956125259399414\n",
      "Epoch 435/500  Running Loss : 38.84941101074219 and test loss: 21.971942901611328\n",
      "Epoch 436/500  Running Loss : 38.8477668762207 and test loss: 21.987751007080078\n",
      "Epoch 437/500  Running Loss : 38.84613800048828 and test loss: 22.00353240966797\n",
      "Epoch 438/500  Running Loss : 38.844520568847656 and test loss: 22.019304275512695\n",
      "Epoch 439/500  Running Loss : 38.842918395996094 and test loss: 22.035057067871094\n",
      "Epoch 440/500  Running Loss : 38.84132766723633 and test loss: 22.050796508789062\n",
      "Epoch 441/500  Running Loss : 38.839752197265625 and test loss: 22.06649398803711\n",
      "Epoch 442/500  Running Loss : 38.83818054199219 and test loss: 22.082195281982422\n",
      "Epoch 443/500  Running Loss : 38.83662414550781 and test loss: 22.097848892211914\n",
      "Epoch 444/500  Running Loss : 38.8350715637207 and test loss: 22.1135196685791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 445/500  Running Loss : 38.83354187011719 and test loss: 22.129133224487305\n",
      "Epoch 446/500  Running Loss : 38.8320198059082 and test loss: 22.144758224487305\n",
      "Epoch 447/500  Running Loss : 38.83050537109375 and test loss: 22.160348892211914\n",
      "Epoch 448/500  Running Loss : 38.829010009765625 and test loss: 22.17591094970703\n",
      "Epoch 449/500  Running Loss : 38.82752227783203 and test loss: 22.191442489624023\n",
      "Epoch 450/500  Running Loss : 38.82603454589844 and test loss: 22.20697021484375\n",
      "Epoch 451/500  Running Loss : 38.824581146240234 and test loss: 22.222454071044922\n",
      "Epoch 452/500  Running Loss : 38.823116302490234 and test loss: 22.237926483154297\n",
      "Epoch 453/500  Running Loss : 38.8216667175293 and test loss: 22.253389358520508\n",
      "Epoch 454/500  Running Loss : 38.82023239135742 and test loss: 22.268815994262695\n",
      "Epoch 455/500  Running Loss : 38.818809509277344 and test loss: 22.284194946289062\n",
      "Epoch 456/500  Running Loss : 38.8173942565918 and test loss: 22.299558639526367\n",
      "Epoch 457/500  Running Loss : 38.81599044799805 and test loss: 22.314889907836914\n",
      "Epoch 458/500  Running Loss : 38.81459426879883 and test loss: 22.3301944732666\n",
      "Epoch 459/500  Running Loss : 38.813209533691406 and test loss: 22.345476150512695\n",
      "Epoch 460/500  Running Loss : 38.81184005737305 and test loss: 22.36073875427246\n",
      "Epoch 461/500  Running Loss : 38.81047439575195 and test loss: 22.375946044921875\n",
      "Epoch 462/500  Running Loss : 38.809120178222656 and test loss: 22.39113426208496\n",
      "Epoch 463/500  Running Loss : 38.80777359008789 and test loss: 22.406280517578125\n",
      "Epoch 464/500  Running Loss : 38.80644607543945 and test loss: 22.421424865722656\n",
      "Epoch 465/500  Running Loss : 38.80511474609375 and test loss: 22.43653106689453\n",
      "Epoch 466/500  Running Loss : 38.80379867553711 and test loss: 22.451595306396484\n",
      "Epoch 467/500  Running Loss : 38.80250549316406 and test loss: 22.466636657714844\n",
      "Epoch 468/500  Running Loss : 38.80119705200195 and test loss: 22.481653213500977\n",
      "Epoch 469/500  Running Loss : 38.79991912841797 and test loss: 22.49663543701172\n",
      "Epoch 470/500  Running Loss : 38.79864501953125 and test loss: 22.51158332824707\n",
      "Epoch 471/500  Running Loss : 38.79736328125 and test loss: 22.526504516601562\n",
      "Epoch 472/500  Running Loss : 38.796104431152344 and test loss: 22.541378021240234\n",
      "Epoch 473/500  Running Loss : 38.79486083984375 and test loss: 22.556224822998047\n",
      "Epoch 474/500  Running Loss : 38.793609619140625 and test loss: 22.571043014526367\n",
      "Epoch 475/500  Running Loss : 38.792381286621094 and test loss: 22.58580780029297\n",
      "Epoch 476/500  Running Loss : 38.79115295410156 and test loss: 22.600570678710938\n",
      "Epoch 477/500  Running Loss : 38.78994369506836 and test loss: 22.615272521972656\n",
      "Epoch 478/500  Running Loss : 38.78873825073242 and test loss: 22.62995719909668\n",
      "Epoch 479/500  Running Loss : 38.78753662109375 and test loss: 22.644622802734375\n",
      "Epoch 480/500  Running Loss : 38.78635025024414 and test loss: 22.65923500061035\n",
      "Epoch 481/500  Running Loss : 38.78517150878906 and test loss: 22.673810958862305\n",
      "Epoch 482/500  Running Loss : 38.78398895263672 and test loss: 22.688365936279297\n",
      "Epoch 483/500  Running Loss : 38.7828254699707 and test loss: 22.702863693237305\n",
      "Epoch 484/500  Running Loss : 38.78167724609375 and test loss: 22.71734046936035\n",
      "Epoch 485/500  Running Loss : 38.780517578125 and test loss: 22.731788635253906\n",
      "Epoch 486/500  Running Loss : 38.779380798339844 and test loss: 22.746173858642578\n",
      "Epoch 487/500  Running Loss : 38.77825164794922 and test loss: 22.760513305664062\n",
      "Epoch 488/500  Running Loss : 38.77713394165039 and test loss: 22.774856567382812\n",
      "Epoch 489/500  Running Loss : 38.77600860595703 and test loss: 22.78914451599121\n",
      "Epoch 490/500  Running Loss : 38.77490234375 and test loss: 22.803403854370117\n",
      "Epoch 491/500  Running Loss : 38.773799896240234 and test loss: 22.817617416381836\n",
      "Epoch 492/500  Running Loss : 38.772705078125 and test loss: 22.831798553466797\n",
      "Epoch 493/500  Running Loss : 38.77162551879883 and test loss: 22.845949172973633\n",
      "Epoch 494/500  Running Loss : 38.770545959472656 and test loss: 22.86005210876465\n",
      "Epoch 495/500  Running Loss : 38.76947784423828 and test loss: 22.874095916748047\n",
      "Epoch 496/500  Running Loss : 38.76841354370117 and test loss: 22.88814926147461\n",
      "Epoch 497/500  Running Loss : 38.767356872558594 and test loss: 22.902141571044922\n",
      "Epoch 498/500  Running Loss : 38.76630783081055 and test loss: 22.91607666015625\n",
      "Epoch 499/500  Running Loss : 38.76526641845703 and test loss: 22.930004119873047\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "for epoch in range(epochs):\n",
    "  running_loss = 0.0\n",
    "  for index in range(0,len(train_batches)):\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(train_batches[index][0]).reshape([-1])\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs,train_batches[index][1])\n",
    "    running_loss += loss\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "  test_accuracy = criterion(model(test_x).reshape([-1]),test_y)\n",
    "  print(f\"Epoch {epoch}/{epochs}  Running Loss : {running_loss.item()/batch_size} and test loss: {test_accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ad352",
   "metadata": {},
   "source": [
    "<h1>Plaintext Inference</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0869e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "plaintext_predictions = model(test_x).reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc3fbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  22.930004119873047\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE Loss: \",criterion(plaintext_predictions,test_y).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b8c96",
   "metadata": {},
   "source": [
    "<h1>Encrypted Inference</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "314395b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Syft and SyMPC imports required for encrypted inference\n",
    "import syft as sy\n",
    "from sympc.module.nn import mse_loss\n",
    "import sympc\n",
    "from sympc.session import Session\n",
    "from sympc.session import SessionManager\n",
    "from sympc.tensor import MPCTensor\n",
    "from sympc.config import Config\n",
    "from sympc.protocol import Falcon,FSS\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e9096e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clients(n_parties):\n",
    "  #Generate required number of syft clients and return them.\n",
    "\n",
    "  parties=[]\n",
    "  for index in range(n_parties): \n",
    "      parties.append(sy.VirtualMachine(name = \"worker\"+str(index)).get_root_client())\n",
    "\n",
    "  return parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a394a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(n_clients,protocol):\n",
    "    \n",
    "  # Get VM clients \n",
    "  parties=get_clients(n_clients)\n",
    "\n",
    "  # Setup the session for the computation\n",
    "  session = Session(parties = parties,protocol = protocol)\n",
    "  SessionManager.setup_mpc(session)\n",
    "\n",
    "  #Encrypt model \n",
    "  mpc_model = model.share(session)\n",
    "\n",
    "  #Encrypt test data\n",
    "  test_data=MPCTensor(secret=test_x, session = session)\n",
    "\n",
    "  #Perform inference and measure time taken\n",
    "  start_time = time.time()\n",
    "  enc_results = mpc_model(test_data)\n",
    "  end_time = time.time()\n",
    "\n",
    "  print(f\"Time for inference: {end_time-start_time}s\")\n",
    "\n",
    "  #Get plaintext predictions\n",
    "  predictions = enc_results.reconstruct()\n",
    "  \n",
    "  #Calculate Loss\n",
    "  print(\"MSE Loss: \",criterion(predictions.reshape([-1]),test_y).item())\n",
    "    \n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d62c0ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.03281688690185547s\n",
      "MSE Loss:  22.929933547973633\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,Falcon(\"semi-honest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5cdd5",
   "metadata": {},
   "source": [
    "We can see that the prediction values and mean squared error values are almost the same as final model. Small differences are due to precision loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6be0ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0\n",
      "Encrypted Prediction Output 1.840240478515625\n",
      "Plaintext Prediction Output 1.8402048349380493\n",
      "Expected Prediction: 5.0\n",
      "\n",
      "\n",
      "Index 1\n",
      "Encrypted Prediction Output 5.1915740966796875\n",
      "Plaintext Prediction Output 5.191580772399902\n",
      "Expected Prediction: 11.8984375\n",
      "\n",
      "\n",
      "Index 2\n",
      "Encrypted Prediction Output 19.7861328125\n",
      "Plaintext Prediction Output 19.786155700683594\n",
      "Expected Prediction: 27.90625\n",
      "\n",
      "\n",
      "Index 3\n",
      "Encrypted Prediction Output 13.1734619140625\n",
      "Plaintext Prediction Output 13.173490524291992\n",
      "Expected Prediction: 17.203125\n",
      "\n",
      "\n",
      "Index 4\n",
      "Encrypted Prediction Output 20.987045288085938\n",
      "Plaintext Prediction Output 20.987075805664062\n",
      "Expected Prediction: 27.5\n",
      "\n",
      "\n",
      "Index 5\n",
      "Encrypted Prediction Output 14.210540771484375\n",
      "Plaintext Prediction Output 14.210532188415527\n",
      "Expected Prediction: 15.0\n",
      "\n",
      "\n",
      "Index 6\n",
      "Encrypted Prediction Output 19.272994995117188\n",
      "Plaintext Prediction Output 19.273014068603516\n",
      "Expected Prediction: 17.203125\n",
      "\n",
      "\n",
      "Index 7\n",
      "Encrypted Prediction Output 1.7180023193359375\n",
      "Plaintext Prediction Output 1.717998743057251\n",
      "Expected Prediction: 17.90625\n",
      "\n",
      "\n",
      "Index 8\n",
      "Encrypted Prediction Output 10.214675903320312\n",
      "Plaintext Prediction Output 10.214686393737793\n",
      "Expected Prediction: 16.296875\n",
      "\n",
      "\n",
      "Index 9\n",
      "Encrypted Prediction Output -7.1717681884765625\n",
      "Plaintext Prediction Output -7.171802043914795\n",
      "Expected Prediction: 7.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(0,10):\n",
    "    print(f\"Index {index}\")\n",
    "    print(f\"Encrypted Prediction Output {predictions[index].item()}\")\n",
    "    print(f\"Plaintext Prediction Output {plaintext_predictions[index].item()}\")\n",
    "    print(f\"Expected Prediction: {test_y[index]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b40e4c",
   "metadata": {},
   "source": [
    "<h1> Conclusion </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc2029",
   "metadata": {},
   "source": [
    "Falcon can also work with malicious security guarantee but at a large inference time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c930a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.5620832443237305s\n",
      "MSE Loss:  22.929933547973633\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,Falcon(\"malicious\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "598ab19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-13T19:26:39.657868+0530][CRITICAL][logger]][32938] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: fbaddfc859fb4a36ace0f4f3fdf82efa>.\n",
      "[2021-07-13T19:26:39.660616+0530][CRITICAL][logger]][32938] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: b087878d1e9542369b7087da48075252>.\n",
      "[2021-07-13T19:26:39.663202+0530][CRITICAL][logger]][32938] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: f150e136b5a94831b5eb8b296f86b40b>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.5415430068969727s\n",
      "MSE Loss:  22.929967880249023\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,FSS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8ceef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-13T19:26:41.369789+0530][CRITICAL][logger]][32938] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: ec3046e80a7149c9830ee8d73af69433>.\n",
      "[2021-07-13T19:26:41.374661+0530][CRITICAL][logger]][32938] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 5a617c91666648e298e42a1c964fe0a7>.\n",
      "[2021-07-13T19:26:41.376340+0530][CRITICAL][logger]][32938] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 3570c369a5ba44c7be3b2b1f0456af00>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.9285099506378174s\n",
      "MSE Loss:  22.929994583129883\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(5,FSS())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04560264",
   "metadata": {},
   "source": [
    "\n",
    "| Protocol | Security Type| Parties | Inference Time (s) |\n",
    "| --- | --- | --- | --- |\n",
    "| Falcon | Semi-honest | 3 | 0.03391 |\n",
    "| Falcon | Malicious | 3 | 0.61146 |\n",
    "| FSS| Semi-honest | 3 | 0.56047 |\n",
    "| FSS | Semi-honest | 5 | 0.91995|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9830545",
   "metadata": {},
   "source": [
    "Falcon provides fast inference for 3 parties in semi-honest setting. While, Functional Secret Sharing (FSS) allows inference for N number of parties. Both allow inference with reasonable accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42f165",
   "metadata": {},
   "source": [
    "<h3>What's next?</h3>\n",
    "\n",
    "SyMPC is still under development! We will add here more features as soon they are stable enough, stay tuned! 🕺\n",
    "\n",
    "If you enjoyed this tutorial, show your support by Starring SyMPC! 🙏"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
